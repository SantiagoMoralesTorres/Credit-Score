---
title: "Credit Score"
author: "Santiago Yael Morales Torres"
date: "2025-07-07"
output: 
    pdf_document:
      extra_dependencies: ["bbm", "subfig","amsmath"]
      number_sections: true
      toc: false
      fig_caption: yes
---

# Contexto y explicación de los datos

La base de datos corresponde al registro de las características demográficas y bancarias de los clientes de un banco, donde cada fila representa un registro mensual por cliente. En total, se tienen 100000 registros correspondientes a los meses de Enero a Agosto de 12500 clientes, y un total de 28 variables. En particular, la variable objetivo de este proyecto es Credit Score, la cual resume el comportamiento del cliente respecto a sus cuentas, transacciones, inversiones, préstamos y deudas, además de dar un indicio del riesgo de inclumpliento en un futuro, y se clasifica en 3 categorías: Good, Standard y Poor. 

Los objetivos de este proyecto son:

- Encontrar relaciones importantes entre las variables de la base y Credit Score.
- Analizar que tan distinguibles o separables son los niveles del Credit Score mediante algoritmos de Aprendizaje No Supervisado.
- Encontrar un modelo de Aprendizaje Supervisado que nos permita predecir con un Accuracy elevado a Credit Score.

A continuación, se explican las variables contenidas en la base de datos (el tipo y valores que se mencionan corresponden a la base de datos después de ser limpiada y transformada):

**ID.** LLave primaria de cada uno de los registros, Identificador único por cliente y mes. *Tipo*: Caracter.

**Customer_ID.** Identificador único del cliente. *Tipo*: Caracter.

**Name.** Nombre Completo del Cliente. *Tipo*: Caracter.

**Month.** Mes correspondiente al registro. *Tipo*: Caracter.

**Age.** Edad del cliente. *Tipo*: Entero.

**SSN.** Número de Seguridad Social. *Tipo*: Caracter.

**Occupation.** Profesión del cliente. *Tipo*: Categórica Nominal con 15 niveles (Accountant, Architect, Developer, Doctor, Engineer, Entrepreneur, Journalist, Lawyer, Manager, Mechanic, Media_Manager, Musician, Scientist, Teacher, Writer).

**Annual_Income.** Ingreso bruto anual del cliente. *Tipo*: Numérica.

**Monthly_Inhand_Salary.** Ingreso neto mensual del cliente (Ingreso bruto mensual menos impuestos y deducciones). *Tipo*: Numérica.

**Num_Bank_Accounts.** Número de cuentas que posee el cliente. *Tipo*: Entero.

**Num_Credit_Card.** Número de tarjetas de crédito que posee el cliente. *Tipo*: Entero.

**Interest_Rate.** Tasa de interés que tienen los préstamos del cliente. *Tipo*: Entero.

**Num_of_Loan.** Número de préstamos que ha solicitado el cliente. *Tipo*: Entero.

**Type_of_Loan.** Tipo de préstamo(s) que ha solicitado el cliente. *Tipo*: Multivaluada/Factor con 8 niveles (Auto Loan, Credit-Builder Loan, Personal Loan, Home Equity Loan, Mortgage Loan, Student Loan, Debt Consolidation Loan, Payday Loan).

**Delay_from_due_date.** Número de días que el cliente se retrasó en pagar respecto a la fecha límite de pago. *Tipo*: Entero.

**Num_of_Delayed_Payment.** Número de pagos que se han realizado después de la fecha límite de pago. *Tipo*: Entero.

**Changed_Credit_Limit.** Variación en el límite de crédito otorgado al cliente en ese mes. *Tipo*: Numérica.

**Num_Credit_Inquiries.** Número de veces que las instituciones financieras han consultado el historial crediticio del cliente. *Tipo*: Entero.

**Credit_Mix.** Califica la variedad de préstamos y créditos que tiene el cliente. *Tipo*: Factor con 3 niveles (Good, Standard, Bad).

**Outstanding_Debt.** Deuda pendiente que tiene el cliente con el banco. *Tipo*: Numérica.

**Credit_Utilization_Ratio.** Porcentaje de crédito utilizado respecto al total de crédito otorgado al cliente. *Tipo*: Numérica.

**Credit_History_Age.** Antigüedad en años del historial crediticio del cliente. *Tipo*: Entero.

**Payment_of_Min_Amount.** Indica si el cliente hizo el pago mínimo o no. *Tipo*: Factor con 2 niveles (Yes, No).

**Total_EMI_per_month.** Total de pagos mensuales fijos del cliente. *Tipo*: Numérica.

**Amount_invested_monthly.** Monto mensual que el cliente invierte mensualmente. *Tipo*: Numérica.

**Payment_Behaviour.** Clasificación del tipo de gastos que hace el cliente y el tipo de pagos que realiza para cubrirlos. *Tipo*: Factor con 6 niveles (Low_spent_Small_value_payments, Low_spent_Medium_value_payments, Low_spent_Large_value_payments, High_spent_Small_value_payments, High_spent_Medium_value_payments, High_spent_Large_value_payments).

**Monthly_Balance.** Saldo mensual del cliente despues de gastos, pagos e inversiones. *Tipo*: Numérica.

**Credit_Score.** Califica el comportamiento financiero del cliente y por tanto el riesgo de incumplimiento.  *Tipo*: Factor con 3 niveles (Good, Standard y Poor).

# Limpieza y transformación de datos

```{r chunks opciones, warning = FALSE, message = FALSE, echo = FALSE, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, include = TRUE)
rm(list = ls(all.names = TRUE))
gc()
```

```{r librerias}
library(dplyr) #agrupación y manipulación de datos
library(kableExtra) #tablas
library(skimr) #Summaries detallados
library(ggplot2) #gráficos 
library(caret) #evaluar poder predictivo (train() y trainControl())
library(tidymodels) #evaluar poder predictivo (model(), recipe(), workflow(), grid_random(), vfold_cv(), tune_grid(), select_best(), finalize_workflow(), fit())
library(glmnet) #regresion logística multinomial lasso
library(MASS) #lda,qda
library(e1071) #naive classifier
library(metrica) #obtener metricas de poder predictivo con metrics_summary
library(nnet) #Redes neuronales de a lo más 1 capa oculta
library(psych) #pca con rotación
library(factoextra) #biplots
library(VGAM) #regresión logística multinomial
library(stringr) #manejo de cadenas string
library(yardstick) #obtener metricas de poder predictivo de manera individual
library(tictoc) # medir tiempo de ejecución
library(doParallel) #paralelizacion de procesos
library(parallel) #contar nucleos
```

```{r carga de datos, include = TRUE}
datos <- read.csv("C:/Users/santi/Documents/Actuaria/6to semestre/Seminario de Estadística/Proyecto/datos.csv")
```

La base de datos tiene una estructura repetitiva cada 8 registros, siendo los primeros 8 registros los correspondientes a los meses de Enero a Agosto del primer cliente, los siguientes 8 a los meses de Enero de Agosto del segundo cliente y así sucesivamente. La siguiente tabla corresponde los primeros 8 registros del primer cliente.

```{r head datos, include = TRUE}
head(datos[,1:8],8) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
head(datos[,9:13],8) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
head(datos[,14:15],8) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
head(datos[,16:20],8) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
head(datos[,21:24],8) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
head(datos[,25:28],8) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```

Se puede observar que ciertas variables se mantienen constantes como Customer_ID, Name, Age, SSN, Occupation, correspondientes a las variables demográficas del cliente, mientras que otras más relacionadas al comportamiento financiero varian cada mes, aunque algunas otras también se mantienen constantes (esto depende de cada cliente). En particular, con solo estos 8 registros ya se observan algunas complicaciones respecto a la integridad de los datos, teniendo valores atípicos o sin sentido (como -500 en Age, #F%$D@*&8 en SSN, -1 en Delay_from_Due_Date, !@9#%8 en Payment_Behaviour, etc.), caracteres que ensucian algunos datos numéricos (_8 en Num_of_Delayed_Payment), campos en blanco, con guiones - o NA (como en Monthly_Inhand_Salary, Changed_Credit_Limit, Credit_Mix, Credit_History_Age), y la variable Type_of_Loan es multivaluada.

Al cargar los datos, inicialmente se tiene la siguiente estructura:

```{r str datos, include = TRUE}
glimpse(datos)
```

Se puede observar que muchas variables numéricas se cargan como tipo char por los caracteres sucios o espacios en blanco que contienen, y algunas variables contienen ciertos niveles y deben ser transformadas a variables de tipo factor. Se procede entonces a realizar el cambio correspondiente al tipo de dato que cada variable debería tener. Para ello, en el caso de las numéricas, se limpian las cadenas de tal manera que solo se conserven dígitos del 0 al 9, puntos decimales y signos negativos (cuando aplique), de tal manera que se logre convertir a numérica sin problema. Algunas variables como Type_of_Loan que son multivaluadas no pueden ser transformdas a tipo factor directamente, por lo que momentáneamente se conserva como tipo char. Por otro lado, la variable Credit_History_Age contiene la antigüedad del historial crediticio en formato A años M meses, por lo que conviene solo obtener los años en tipo numérico, extrayendo los 2 primeros dígitos de la cadena y transformando de igual manera a variable numérica.


```{r transformación de datos, include = TRUE}
#Conversión al tipo de dato correspondiente
datos$ID <- as.character(datos$ID)
datos$Customer_ID <- as.character(datos$Customer_ID)
datos$Month<- as.factor(datos$Month)
datos$Name <- as.character(datos$Name)
datos$Age <-  as.numeric(gsub("[^0-9]", "", datos$Age)) #quitar caracteres atipicos que no sean digitos
datos$SSN <- as.character(datos$SSN)
datos$Occupation <- as.factor(datos$Occupation)
datos$Annual_Income <-  as.numeric(gsub("[^0-9.]", "", datos$Annual_Income)) #quitar caracteres atipicos que no sean digitos o punto decimal
datos$Monthly_Inhand_Salary <-  as.numeric(gsub("[^0-9.]", "", datos$Monthly_Inhand_Salary))
datos$Num_Bank_Accounts <-  as.numeric(gsub("[^0-9.]", "", datos$Num_Bank_Accounts))
datos$Num_Credit_Card <-  as.numeric(gsub("[^0-9.]", "", datos$Num_Credit_Card))
datos$Interest_Rate <-  as.numeric(gsub("[^0-9.]", "", datos$Interest_Rate))
datos$Num_of_Loan <-  as.numeric(gsub("[^0-9.]", "", datos$Num_of_Loan))
datos$Delay_from_due_date <-  as.numeric(gsub("[^0-9.]", "", datos$Delay_from_due_date))
datos$Num_of_Delayed_Payment <-  as.numeric(gsub("[^0-9]", "", datos$Num_of_Delayed_Payment))
datos$Changed_Credit_Limit <-  as.numeric(gsub("[^0-9.-]", "", datos$Changed_Credit_Limit)) #quitar caracteres atipicos que no sean digitos o punto decimal o signo negativo
datos$Num_Credit_Inquiries <-  as.numeric(gsub("[^0-9]", "", datos$Num_Credit_Inquiries))
datos$Credit_Mix <- as.factor(datos$Credit_Mix)
datos$Outstanding_Debt <-  as.numeric(gsub("[^0-9.]", "", datos$Outstanding_Debt))
datos$Credit_Utilization_Ratio <-  as.numeric(gsub("[^0-9.]", "", datos$Credit_Utilization_Ratio))
datos$Credit_History_Age <- as.numeric(substr(datos$Credit_History_Age, start = 1, stop = 2))
datos$Payment_of_Min_Amount <- as.factor(datos$Payment_of_Min_Amount)
datos$Total_EMI_per_month <-  as.numeric(gsub("[^0-9.]", "", datos$Total_EMI_per_month))
datos$Amount_invested_monthly <-  as.numeric(gsub("[^0-9.]", "", datos$Amount_invested_monthly))
datos$Payment_Behaviour <- as.factor(datos$Payment_Behaviour)
datos$Monthly_Balance <-  as.numeric(gsub("[^0-9.]", "", datos$Monthly_Balance))
datos$Credit_Score <- as.factor(datos$Credit_Score)
```

```{r str datos 1, include = TRUE}
glimpse(datos)
```

Se observa una estructura más limpia respecto al tipo de dato, pero nótese que seguimos con algunos valores nulos y atípicos tanto en las variables categóricas como en las numéricas, esto se puede observar de mejor manera en el resumen de los datos por variable.

```{r summary datos 1, include = TRUE}
#obtenemos el resumen de todas las variables
summary_datos <- as.data.frame(skim(datos))
colnames(summary_datos) <- c('Tipo','Variable','NAs','%Sin NAs',
                              'character.min','character.max','Cadenas.Vacias',
                              'Cadenas.Unicas','Espacios.Blanco','factor.ordered',
                              'Num.Clases','Conteo.Clase','Media','Desv.Std',
                              'Min','1st.Qu','Mediana','3rd.Qu','Max','Hist')
summary_datos[,13:19] <- round(summary_datos[,13:19],2)
summary_datos[-28,13:19] <- format(summary_datos[-28,13:19], scientific = FALSE)

#variables tipo character
summary_datos[1:5,c(2,1,3,7,8,9)] %>% 
  kbl(booktabs = TRUE, align = "c", row.names = FALSE) %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))

#variables numéricas
summary_datos[12:28,c(2,1,3,13,14)] %>% 
  kbl(booktabs = TRUE, align = "c", row.names = FALSE) %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))

summary_datos[12:28,c(2,1,15:19)] %>% 
  kbl(booktabs = TRUE, align = "c", row.names = FALSE) %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))

#variables factor
summary_datos[6:11,c(2,1,3,11,12)] %>% 
  kbl(booktabs = TRUE, align = "c", row.names = FALSE) %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```

Muchas variables como Monthly_Inhand_Salary, Num_of_Delayed_Payment, Changed_Credit_Limit, Num_Credit_Inquiries, Credit_History_Age, Amount_invested_monthly y Monthly_Balance tienen una cantidad considerable de NA's (entre 1% y 10%). 

Es notorio que algunas variables tienen evidentes errores de captura que provocan outliers sin sentido como Age (Max = 8698), Num_Bank_Accounts (Max = 1798), Num_Credit_Card (Max = 1499), Interest_Rate (Max = 5797), Num_of_Loan (Max = 1496), Num_of_Delayed_Payment (Max = 4397), Num_Credit_Inquiries (Max = 2597), Monthly_Balance (Max = 3x10^26). 

Otras variables tienen un valor máximo que difiere mucho del tercer cuartil pero que son posibles aunque extraños como Annual Income (Max = 24198062 con respecto a  3rd Qu. = 72791), Monthly_Inhand_Salary (Max = 15204 respecto a 3rd Qu. = 5957.4), Delay_from_due_date (Max = 67 respecto a 3rd Qu.= 28.0), Total_EMI_per_month (Max = 82331 respecto a 3rd Qu. = 161.22) y Amount_invested_monthly (Max = 10000 respecto a 3rd Qu. =  265.73).

Por otro lado, respecto a las variables categóricas y de tipo caracter, algunas tienen NA's silenciosos como Name y Type_of_Loan con espacios en blanco, SSN con caracteres tipo #F%$D@*&8, Occupation con _______, Credit_Mix con _ , Payment_of_Min_Amount con NM (Not Mentioned) y Payment_Behaviour con !@9#%8.

La estrategia para limpiar los datos fue utilizar el hecho de que cada 8 registros tenemos variables que tienen valores idénticos, similares, o con varianza baja debido a que se tratan de un mismo cliente, por lo que se realizó una agrupación por cliente y se obtuvo la moda para varibales categóricas y la mediana para variables numéricas para sustituir los NA's explícitos e implícitos y los valores atípicos. En casos muy particulares donde la moda o mediana fuera el valor atípico o NA (por ejemplo, que en Occupation de los 8 registros 6 tuviesen _______ y solo 2 la ocupación real, o que en age la mediana superara el valor 100), se tomaba entonces el segundo valor más repetido o la mediana general de los datos para imputar.

En particular, los criterios específicos que se tomaron para limpiar cada una de las variables fueron los siguientes:

**ID.** Sin limpieza.

**Customer_ID.** Sin limpieza.

**Name.** Sustituir aquellos valores con cadenas vacías.

**Month.** Sin limpieza.

**Age.** Cambiar aquellos valores que tuviesen un valor menor a 0 o mayor a 100 por la mediana.

**SSN.** Sustituir aquellos valores con la cadena #F%$D@*&8 por la moda.

**Occupation.** Sustituir aquellos valores con la cadena _______ por la moda.

**Annual_Income.** Sustituir aquellos valores mayores a 200000 con la mediana.

**Monthly_Inhand_Salary.** Sustituir los NA's con la mediana.

**Num_Bank_Accounts.** Sustituir aquellos valores mayores a 11 con la mediana.

**Num_Credit_Card.** Sustituir aquellos valores mayores a 11 con la mediana.

**Interest_Rate.** Sustituir aquellos valores mayores a 34 con la mediana.

**Num_of_Loan.** Sustituir aquellos valores mayores a 9 con la mediana..

**Type_of_Loan.** Sin limpieza (por el momento).

**Delay_from_due_date.** Sin limpieza.

**Num_of_Delayed_Payment.** Sustituir aquellos valores mayores a 28 con la mediana.

**Changed_Credit_Limit.** Sustituir los NA's con la mediana.

**Num_Credit_Inquiries.** Sustituir los NA's y aquellos valores mayores a 17 con la mediana.

**Credit_Mix.** Sustituir aquellos valores con la cadena _ por la moda.

**Outstanding_Debt.** Sin Limpieza.

**Credit_Utilization_Ratio.** Sin Limpieza.

**Credit_History_Age.** Sustituir los NA's con la mediana.

**Payment_of_Min_Amount.** Sustituir aquellos valores con la cadena NM por la moda.

**Total_EMI_per_month.** Sustituir aquellos valores mayores a 1800 con la mediana.

**Amount_invested_monthly.** Sustituir los NA's y aquellos valores mayores a 2000 con la mediana.

**Payment_Behaviour.** Sustituir aquellos valores con la cadena !@9#%8 por la moda.

**Monthly_Balance.** Sustituir los NA's y aquellos valores mayores a 2000 con la mediana.

**Credit_Score.** Sin Limpieza.


```{r limpieza de datos, include = TRUE}
#Limpieza de datos (quitar valores nulos y/u outliers de las variables necesarias)
datos$Occupation <- as.character(datos$Occupation)
datos$Credit_Mix <- as.character(datos$Credit_Mix)
datos$Payment_of_Min_Amount <- as.character(datos$Payment_of_Min_Amount)
datos$Payment_Behaviour <- as.character(datos$Payment_Behaviour)


datos <- datos %>%
  group_by(Customer_ID) %>% #agrupar por cliente
  mutate(Name = ifelse(Name == "", #si el valor es vacío
                ifelse(names(which.max(table(Name))) == "", #si el valor más repetido es también vacío
                       names(sort(table(Name), decreasing = TRUE))[2], #reemplazar por el 2do valor más repetido si el primero también es vacío
                             names(which.max(table(Name)))), #reemplazar por el valor más repetido
                      Name),
         Age = ifelse(is.na(Age) | Age < 0 | Age > 100,  #edades NA o en valores no consistentes
                      as.numeric(names(which.max(table(Age)))), #reemplazar por el valor más repetido
                      Age),
         SSN = ifelse(SSN == "#F%$D@*&8",  
                      names(which.max(table(SSN))), #reemplazar por el valor más repetido
                      SSN),
         Occupation = ifelse(Occupation == "_______",
                      ifelse(names(which.max(table(Occupation))) == "_______", #si el valor más repetido es también vacío
                       names(sort(table(Occupation), decreasing = TRUE))[2], #reemplazar por el 2do valor más repetido si el primero también es vacío
                             names(which.max(table(Occupation)))), #reemplazar por el valor más repetido      
                      Occupation),
         Annual_Income = ifelse(is.na(Annual_Income) | Annual_Income > 200000,  
                      median(Annual_Income, na.rm = TRUE), #reemplazar por la mediana
                      Annual_Income),
         Monthly_Inhand_Salary = ifelse(is.na(Monthly_Inhand_Salary),  
                      median(Monthly_Inhand_Salary, na.rm = TRUE), #reemplazar por la mediana
                      Monthly_Inhand_Salary),
         Num_Bank_Accounts = ifelse(is.na(Num_Bank_Accounts) | Num_Bank_Accounts > 11, 
                      median(Num_Bank_Accounts, na.rm = TRUE), #reemplazar por la mediana
                      Num_Bank_Accounts),
         Num_Credit_Card = ifelse(is.na(Num_Credit_Card) | Num_Credit_Card > 11, 
                      median(Num_Credit_Card, na.rm = TRUE), #reemplazar por la mediana
                      Num_Credit_Card),
         Interest_Rate = ifelse(is.na(Interest_Rate) | Interest_Rate > 34, 
                      median(Interest_Rate, na.rm = TRUE), #reemplazar por la mediana
                      Interest_Rate),
         Num_of_Loan = ifelse(is.na(Num_of_Loan) | Num_of_Loan > 9, 
                      ifelse(median(Num_of_Loan, na.rm = TRUE) > 9,
                             min(Num_of_Loan, na.rm = TRUE), #reemplazar por el valor minimo
                             median(Num_of_Loan, na.rm = TRUE)), #reemplazar por la mediana        
                      Num_of_Loan),
         Num_of_Delayed_Payment = ifelse(is.na(Num_of_Delayed_Payment) | Num_of_Delayed_Payment > 28, 
                      median(Num_of_Delayed_Payment, na.rm = TRUE), #reemplazar por la mediana
                      Num_of_Delayed_Payment),
         Changed_Credit_Limit = ifelse(is.na(Changed_Credit_Limit), 
                      median(Changed_Credit_Limit, na.rm = TRUE), #reemplazar por la mediana
                      Changed_Credit_Limit),
         Num_Credit_Inquiries = ifelse(is.na(Num_Credit_Inquiries) | Num_Credit_Inquiries > 17, 
                      median(Num_Credit_Inquiries, na.rm = TRUE), #reemplazar por la mediana
                      Num_Credit_Inquiries),
         Credit_Mix = ifelse(Credit_Mix == "_",
                      ifelse(names(which.max(table(Credit_Mix))) == "_",
                             names(sort(table(Credit_Mix), decreasing = TRUE))[2], #reemplazar por el 2do valor más repetido si el primero también es vacío
                             names(which.max(table(Credit_Mix)))) #reemplazar por el valor más repetido 
                      ,Credit_Mix),
         Credit_History_Age = ifelse(is.na(Credit_History_Age), 
                      median(Credit_History_Age, na.rm = TRUE), #reemplazar por la mediana
                      Credit_History_Age),
         Payment_of_Min_Amount = ifelse(Payment_of_Min_Amount == "NM",
                      ifelse(names(which.max(table(Payment_of_Min_Amount))) == "NM",
                             names(sort(table(Payment_of_Min_Amount), decreasing = TRUE))[2], #reemplazar por el 2do valor más repetido si el primero también es vacío
                             names(which.max(table(Payment_of_Min_Amount)))) #reemplazar por el valor más repetido
                      ,Payment_of_Min_Amount),
         Total_EMI_per_month = ifelse(is.na(Total_EMI_per_month) | Total_EMI_per_month > 1800, 
                      median(Total_EMI_per_month, na.rm = TRUE), #reemplazar por la mediana
                      Total_EMI_per_month),
         Amount_invested_monthly = ifelse(is.na(Amount_invested_monthly) | Amount_invested_monthly > 2000, 
                      median(Amount_invested_monthly, na.rm = TRUE), #reemplazar por la mediana
                      Amount_invested_monthly),
         Payment_Behaviour = ifelse(Payment_Behaviour == "!@9#%8",
                      ifelse(names(which.max(table(Payment_Behaviour))) == "!@9#%8",
                             names(sort(table(Payment_Behaviour), decreasing = TRUE))[2], #reemplazar por el 2do valor más repetido si el primero también es vacío
                             names(which.max(table(Payment_Behaviour)))) #reemplazar por el valor más repetido
                      ,Payment_Behaviour),
         Monthly_Balance = ifelse(is.na(Monthly_Balance) | Monthly_Balance > 2000, 
                      median(Monthly_Balance, na.rm = TRUE), #reemplazar por la mediana
                      Monthly_Balance)
         ) %>%
  ungroup()
  
#convertir a factor las variables correspondientes
datos$Occupation <- as.factor(datos$Occupation) 
datos$Credit_Mix <- as.factor(datos$Credit_Mix) 
datos$Payment_of_Min_Amount <- as.factor(datos$Payment_of_Min_Amount)
datos$Payment_Behaviour <- as.factor(datos$Payment_Behaviour) 

#datos_limpios <- write.csv(datos,"C:\Users\santi\Documents\Actuaria\6to semestre\Seminario de Estadística\Proyecto\datos_limpios.csv")
```

Una vez realizada la limpieza de los datos, obtenemos la siguiente estructura.

```{r str datos 2, include = TRUE}
glimpse(datos)
```

La estructura de los datos ya considera el tipo de variable adecuado para casi todas las variables de la base de datos (excepto Type_of_Loan). Revisando el resumen de los datos obtenemos lo siguiente.

```{r summary datos 2, include = TRUE}
#obtenemos el resumen de todas las variables
summary_datos <- as.data.frame(skim(datos))
colnames(summary_datos) <- c('Tipo','Variable','NAs','%Sin NAs',
                              'character.min','character.max','Cadenas.Vacias',
                              'Cadenas.Unicas','Espacios.Blanco','factor.ordered',
                              'Num.Clases','Conteo.Clase','Media','Desv.Std',
                              'Min','1st.Qu','Mediana','3rd.Qu','Max','Hist')
summary_datos[,13:19] <- round(summary_datos[,13:19],2)
summary_datos[-28,13:19] <- format(summary_datos[-28,13:19], scientific = FALSE)

#variables tipo character
summary_datos[1:5,c(2,1,3,7,8,9)] %>% 
  kbl(booktabs = TRUE, align = "c", row.names = FALSE) %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))

#variables numéricas
summary_datos[12:28,c(2,1,3,13,14)] %>% 
  kbl(booktabs = TRUE, align = "c", row.names = FALSE) %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))

summary_datos[12:28,c(2,1,15:19)] %>% 
  kbl(booktabs = TRUE, align = "c", row.names = FALSE) %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))

#variables factor
summary_datos[6:11,c(2,1,3,11,12)] %>% 
  kbl(booktabs = TRUE, align = "c", row.names = FALSE) %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```


El resumen de los datos también muestra que la base de datos se limpió correctamente, ya no hay datos NA's explícitos, NA's implícitos en las variables categóricas, ni outliers en las variables numéricas. El último paso para tener una base de datos completamente limpia y lista para el análisis es considerar las variables importantes, transformar la variable Type_of_Loan, y convertir las variables categóricas a variables tipo dummie.

Las variables ID, Customer_ID, Month, Name y SSN son variables específicas del cliente y la fecha, las cuales son completamente independientes a como es el comportamiento financiero del cliente, por ser identificadores únicos del cliente y tiempo, por lo que no se consideran para el análisis. 

Por otro lado, de las variables categóricas, las variables Credit_Mix y Payment_of_Min_Amount son del tipo categórica ordinal, por lo que pueden ser codificadas de manera numérica. Por tanto, para el caso de Credit_Mix se transformó a los niveles Good, Standard y Poor a los números 3,2 y 1 respectivamente, mientras que para Payment_of_Min_Amount con niveles Yes y No se codificó como 1 y 0 respectivamente. 

Las variables Occupation y Payment_Behaviour son de tipo categórica nominal, por lo que en este caso se considera una nueva variable por nivel con valor 1 si el cliente tiene ese nivel y 0 si no. 

Finalmente, la variable Type_of_Loan al ser de tipo mulivaluada se separa de igual manera como variables dummies. Para ello a las cadenas del estilo "Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan" se les aplica un separador, usando "," o "and" como separadores, y se eliminan los valores "Not Specified" o las cadenas vacías. Así, se tiene la estructura final siguiente.

```{r transformacion categoricas a numericas, include = TRUE}
#quitar variables no informativas (ID, Customer_ID, Month, Name, SSN)
datos_est <- datos
datos <- datos[,-c(1,2,3,4,6)] 

#variables categóricas ordinales
datos$Credit_Mix <- ifelse(datos$Credit_Mix == "Bad", 1, 
                           ifelse(datos$Credit_Mix == "Standard", 2, 3))
datos$Payment_of_Min_Amount <- ifelse(datos$Payment_of_Min_Amount == "Yes", 1, 0)
datos$Credit_Score <- factor(datos$Credit_Score, levels = c("Poor","Good","Standard"))

#variables categóricas nominales
matrix_Occupation <- model.matrix(~ Occupation - 1, data = datos)
colnames(matrix_Occupation) <- sub("^Occupation", "", colnames(matrix_Occupation))
matrix_Payment_Behaviour <- model.matrix(~ Payment_Behaviour - 1, data = datos)
colnames(matrix_Payment_Behaviour) <- sub("^Payment_Behaviour", "", colnames(matrix_Payment_Behaviour))

#variables multivaluadas
datos$Type_of_Loan <- str_replace_all(datos$Type_of_Loan, " and ", ", ")
Loans <- unique(unlist(strsplit(datos$Type_of_Loan, ",\\s*")))
Loans <- Loans[Loans != "" & Loans != "Not Specified"]
matrix_Type_of_Loan <- matrix(nrow=nrow(datos),ncol=length(Loans))
colnames(matrix_Type_of_Loan) <- Loans
for(Loan in Loans) {
  matrix_Type_of_Loan[,Loan] <- ifelse(str_detect(datos$Type_of_Loan, fixed(Loan)), 1, 0)
}

#unir las variables dummies
datos <- datos[,-c(2,9,21)] #quitar occupation, Type_of_Loan, Payment_Bahaviour (categóricas)
datos <- cbind(datos[,setdiff(names(datos), "Credit_Score")],
               matrix_Occupation,
               matrix_Type_of_Loan,
               matrix_Payment_Behaviour,
               datos[,"Credit_Score"])
```

```{r str datos final, include = TRUE}
glimpse(datos)
```


De esta manera, obtenemos finalmente un dataframe con 49 variables, de las cuales 19 son numéricas, 29 son variables dummies y 1 (Credit_Score) es la variables objetivo.

# Estadística Descriptiva de los datos respecto a la variable Credit Score

Los siguientes boxplots muestran el comportamiento de cada una de las variables numéricas de la base de datos agrupadas por Credit Score.

```{r variables numericas, include = TRUE, fig.height=2, fig.width=6, fig.align='center'}
datos_est$Credit_Score <- factor(datos_est$Credit_Score, 
                                 levels = c('Poor','Standard','Good'))
#Age
ggplot(datos_est, aes(x = Credit_Score, y = Age, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Age",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip(ylim = c(12, 60))

#Annual_Income
ggplot(datos_est, aes(x = Credit_Score, y = Annual_Income, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Annual_Income",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Monthly_Inhand_Salary
ggplot(datos_est, aes(x = Credit_Score, y = Monthly_Inhand_Salary, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Monthly_Inhand_Salary",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Num_Bank_Accounts
ggplot(datos_est, aes(x = Credit_Score, y = Num_Bank_Accounts, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Num_Bank_Accounts",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Num_Credit_Card
ggplot(datos_est, aes(x = Credit_Score, y = Num_Credit_Card, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Num_Credit_Card",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Interest_Rate
ggplot(datos_est, aes(x = Credit_Score, y = Interest_Rate, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Interest_Rate",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Num_of_Loan
ggplot(datos_est, aes(x = Credit_Score, y = Num_of_Loan, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Num_of_Loan",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Delay_from_due_date
ggplot(datos_est, aes(x = Credit_Score, y = Delay_from_due_date, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Delay_from_due_date",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Num_of_Delayed_Payment
ggplot(datos_est, aes(x = Credit_Score, y = Num_of_Delayed_Payment, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Num_of_Delayed_Payment",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Changed_Credit_Limit
ggplot(datos_est, aes(x = Credit_Score, y = Changed_Credit_Limit, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Changed_Credit_Limit",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Num_Credit_Inquiries
ggplot(datos_est, aes(x = Credit_Score, y = Num_Credit_Inquiries, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Num_Credit_Inquiries",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Outstanding_Debt
ggplot(datos_est, aes(x = Credit_Score, y = Outstanding_Debt, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Outstanding_Debt",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Credit_Utilization_Ratio
ggplot(datos_est, aes(x = Credit_Score, y = Credit_Utilization_Ratio, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Credit_Utilization_Ratio",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Credit_History_Age
ggplot(datos_est, aes(x = Credit_Score, y = Credit_History_Age, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Credit_History_Age",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip()

#Total_EMI_per_month
ggplot(datos_est, aes(x = Credit_Score, y = Total_EMI_per_month, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Total_EMI_per_month",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip(ylim = c(0, 500))

#Amount_invested_monthly
ggplot(datos_est, aes(x = Credit_Score, y = Amount_invested_monthly, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Amount_invested_monthly",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip(ylim = c(0, 500))

#Monthly_Balance
ggplot(datos_est, aes(x = Credit_Score, y = Monthly_Balance, fill = Credit_Score)) +
  geom_boxplot() +
  scale_fill_manual(values = c("Good" = "palegreen3","Standard" = "skyblue3",
                                "Poor" = "indianred3")) +
  labs(title = "Monthly_Balance",x = "",y = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none") +
  coord_flip(ylim = c(0, 1000))
```
Se observa como las variables Age, Annual_Income, Monthly_Inhand_Salary, Credit_History_Age, Amount_Invested_Monthly y Monthly_Balance tienen una relación proporcional respecto a la puntuación crediticia, dónde el grupo Good posee los quantiles más altos mientras que el grupo Poor tiene los valores más bajos, y el grupo Standard es un punto intermedio entre ambos. De estas variables, Credit_History_Age muestra la diferenciación más grande entre clases, con una mediana de alrededor de 23 años de historial crediticio, mientras que los grupos Standard y Poor tienen una mediana de 17 años y 12 años respectivamente, y Age al ser una variable relacionada muestra un patrón similar aunque no tan predominante, ya que una persona puede tener una edad avanzada pero no necesariamente haber desarrollado historial, pero evidentemente los del grupo Good al tener una mediana de 23 años de historial crediticio deben ser personas de edades más avanzadas. Por otro lado, las variables Annual_Income, Monthly_Inhand_Salary, Amount_Invested_Monthly y Monthly_Balance se relacionan con el perfil económico del cliente, teniendo entonces que aquellos que suelen tener una buena puntuación crediticia, además de un largo historial, poseen una solvencia económica elevada que permite el pago de los préstamos y créditos que solicitan al banco además de lograr más inversiones.

Por otro lado, tenemos que Num_Bank_Accounts, Num_Credit_Card, Interest_Rate, Num_of_Loan, Delay_from_due_date, Num_of_Delayed_Payment, Num_Credit_Inquiries y Outstanding_Debt muestran una relación inversamente proporcional al Credit Score, donde a mayores valores de estas variables el puntaje es más bajo. Estas variables miden la responsabilidad que tiene el cliente respecto a sus finanzas y el pago de sus préstamos y créditos, y en este caso si se muestran diferencias muy significativas principalmente entre los grupos Poor y Good con respecto a estas variables, sobretodo en aquellas que están relacionadas directamente con los préstamos como son Interest_Rate, Num_of_Loan, Delay_from_due_date, Num_of_Delayed_Payment y Outstanding_Debt, donde el 1er cuantil en el grupo Poor es incluso mayor o igual al 3er cuantil del grupo Good. Esta diferencia se vuelve más grande ya que, a diferencia de las variables que benefician al grupo Good, los préstamos, retrasos, deuda y tasas de interés aumentan con mucha facilidad si el cliente no es responsable con los pagos de los préstamos, a diferencia de la posición económica y la antigüedad del historial, los cuales toman mucho tiempo aumentar.

Respecto a las variables categóricas, podemos observar las proporciones de cada una por nivel de Credit Score en los siguientes gráficos.

```{r variables categoricas, include = TRUE, fig.height=4, fig.width=7, fig.align='center'}
#Occupation
ggplot(datos_est, aes(x = Credit_Score, fill = Occupation)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(
    title = "Occupation",
    x = "Credit_Score",
    y = "",
    fill = "Occupation"
  ) +
  theme_minimal()


#Credit_Mix
ggplot(datos_est, aes(x = Credit_Score, fill = 
                      factor(Credit_Mix, 
                             levels = c("Good", "Standard", "Bad")))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(
    values = c("Bad" = "indianred3", "Standard" = "skyblue3", "Good" = "palegreen3")
  ) +
  labs(
    title = "Credit_Mix",
    x = "Credit_Score",
    y = "",
    fill = "Credit_Mix"
  ) +
  theme_minimal()

#Payment_of_Min_Amount
ggplot(datos_est, aes(x = Credit_Score, fill = 
                      factor(Payment_of_Min_Amount, 
                             levels = c("No", "Yes")))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(
    values = c("No" = "palegreen3", "Yes" = "indianred3")
  ) +
  labs(
    title = "Payment_of_Min_Amount",
    x = "Credit_Score",
    y = "",
    fill = "Payment_of_Min_Amount"
  ) +
  theme_minimal()

#Payment_Bahaviour
ggplot(datos_est, aes(x = Credit_Score, fill = 
                      factor(Payment_Behaviour, 
                             levels = c("High_spent_Large_value_payments", 
                                        "High_spent_Medium_value_payments",
                                        "High_spent_Small_value_payments",
                                        "Low_spent_Large_value_payments",
                                        "Low_spent_Medium_value_payments",
                                        "Low_spent_Small_value_payments")))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_manual(
    values = c("Low_spent_Small_value_payments" = "indianred3",
               "Low_spent_Medium_value_payments" = "indianred1",
               "Low_spent_Large_value_payments" = "skyblue3",
               "High_spent_Small_value_payments" = "skyblue1",
               "High_spent_Medium_value_payments" = "palegreen3",
               "High_spent_Large_value_payments" = "palegreen1")
  ) +
  labs(
    title = "Payment_Behaviour",
    x = "Credit_Score",
    y = "",
    fill = "Payment_Behaviour"
  ) +
  theme_minimal()
```
La profesión de los clientes no tiene alguna distribución preferencial por nivel de Credit Score, se observa que en general esta variable esta equilibrada en las 3 clases por lo que podríamos asumir que no hay impacto de la profesión en la puntuación crediticia. Sin embargo, las otras 3 variables Credit_Mix, Payment_of_Min_Amount y Payment_Behaviour si tienen una cierta tendencia para cada nivel. 

Credit_Mix tiene una relación sumamente directa con Credit_Score, justamente tenemos que la proporción más dominante en cada nivel de Credit_Score es la misma que en Credit_Mix (Bad-Poor, Standard-Standard y Good-Good), de donde el grupo Good es el que muestra la relación más fuerte con su análogo de Credit_Mix. 

Respecto a Payment_of_Min_Amount, se observa que de aquellos del grupo Poor, la mayoría suelen apenas cubrir el pago mínimo requerido en sus préstamos, mientras que los del grupo Good la mayoría hacen depósitos mayores al mínimo requerido, lo cual es congruente con el hecho de que el grupo Good, al tener un nivel económico más alto y deudas más bajas, tienen más facilidad de cubrir sus deudas a diferencia del grupo Poor, que suelen tener un nivel económico bajo y deudas y tasas de interés altas. 

Finalmente, la variable Payment_Behaviour, a diferencia de Credit_Mix y Payment_of_Min_Amount, no muestra una relación sumamente predominante en cada clase, sin embargo, si es notorio que ciertos comportamientos de pago se asocian ligeramente a cada nivel de Credit Score, ya que los clientes con un Credit Score Good suelen tener gastos elevados pero que a la vez se cubren con pagos de gran cantidad, mientras que los del grupo Poor realizan compras de un valor más bajo y aún así realizan pagos pequeños para pagarlas. Esto tiene la misma explicación que en Payment_of_Min_Amount, aunque más relacionado al nivel económico del cliente y la manera en que el cliente paga cierto tipo de gastos.

En general, se puede concluir que cada puntuación creditica tiene comportamientos destacados en la mayoría de las variables de la base de datos, que se pueden resumir como:

- *Antigüedad del cliente y su historial* (Good tiene más años y Poor pocos años)
- *Posición económica del cliente* (Good tiene una posición alta y Poor una posición baja)
- *Deudas y préstamos del cliente* (Poor tiene un valor elevado y Good un valor bajo)
- *Conducta financiera del cliente* (Poor muestra una conducta irresponsable y Good una conducta responsable)

# Aprendizaje No Supervisado de los datos

A pesar de ya contar con una variable clasificadora y por tanto, ser un problema de aprendizaje supervisado, en esta sección se busca explorar que tan separables o distinguibles son las clases del Credit Score (Good, Standard y Poor) para algoritmos no supervisados como PCA, K-Means y Métodos Jerárquicos. 

## Análisis de Componentes Principales (PCA)

Se ajustó un PCA considerando solamente las variables de tipo numérico de la base de datos, con estandarización de los datos, debido a que si se realizaba el PCA sin ese preprocesamiento de los datos, las variable monetaria Annual_Income era la dominante en el PCA.

El siguiente gráfico muestra la cantidad de varianza explicada por los primeros 10 componentes.

```{r pca, include = TRUE, fig.height=2, fig.width=5, fig.align='center'}
pca <- prcomp(datos[,-c(20:49)], scale=TRUE) #ajuste pca con datos estandarizados

fviz_eig(pca) #barchart de varianza explicada
```

El primer componente principal representa poco más del 40% de la varianza explicada mientras que el segundo componente suma alrededor de un 13%, teniendo un total de 53% de varianza explicada por ambos, lo cuál es un buen número si nuestro objetivo es visualizar en menos dimensiones a las observaciones y las variables.

El siguiente biplot resume de manera gráfica como es que interactúan cada uno de los grupos de Credit Score con los primeros 2 componentes y las variables de la base de datos.

```{r pca biplot, include = TRUE, fig.height=3.5, fig.width=7, fig.align='center'}
set.seed(1)
#seleccionar una muestra representativa para graficar
indices <- as.character(c(sample(which(datos$Credit_Score == "Good"), 1800), 
                          sample(which(datos$Credit_Score == "Standard"), 5000), 
                          sample(which(datos$Credit_Score == "Poor"), 1200)))

fviz_pca_biplot(pca,labelsize = 3, #biplot
                title="PCA - Score Crediticio", 
                ggtheme = theme_minimal(), 
                habillage = datos$Credit_Score, 
                geom = "point", pointsize = 1, 
                xlim = c(-7.5,7.5), ylim = c(-6,4.5), 
                select.ind = list(name = indices),
                addEllipses=TRUE, ellipse.level=0.95, 
                palette = c("indianred3","palegreen3","skyblue2"),
                col.var = "black")
```

Se observa una relación muy importante entre el Credit Score y el componente principal 1, debido a que aquellos con una puntuación crediticia Poor se encuentran mayoritariamente en el valor negativo del PC1, los del grupo Good por el contrario se posicionan en los valores positivos, y los del grupo Standard están alrededor del 0, aunque este ultimo grupo también se mezcla considerablemente con las otras 2 puntuaciones. Del biplot podemos concluir que los grupos no son fácilmente separables, ya que incluso varias observaciones del grupo Poor se encuentran en la zona del grupo Good, y como ya se mencionó antes, los del grupo Standard también se encuentran en zonas tanto del grupo Poor como del grupo Good. Sin embargo, los del grupo Good si se encuentran mayoritariamente ubicados en una zona específica, y no es tan evidente que se fuguen en los otros 2 grupos, sobre todo en el grupo Poor.

Respecto a las variables, los siguientes biplots muestran a detalle cuáles son las variables relacionadas con el grupo Good y cuáles con el grupo Poor.

```{r pca por grupo, include = TRUE, fig.height=3.5, fig.width=7, fig.align='center'}
#Biplot con las variables más significativas del grupo Good
fviz_pca_biplot(pca,labelsize = 3,
                title="PCA - Credit Score Good", 
                ggtheme = theme_minimal(), 
                habillage = datos$Credit_Score, 
                geom = "point", pointsize = 1, 
                xlim = c(0,7.5), ylim = c(-6,3.5),
                select.ind = list(name = indices),
                addEllipses=TRUE, ellipse.level=0.95, 
                palette = c("indianred3","palegreen3","skyblue2"),
                col.var = "black",
                select.var = list(name = c("Credit_Mix","Credit_History_Age",
                                           "Monthly_Balance","Annual_Income",
                                           "Age","Amount_invested_monthly")))

#Biplot con las variables más significativas del grupo Poor
fviz_pca_biplot(pca,labelsize = 3,
                title="PCA - Credit Score Poor", 
                ggtheme = theme_minimal(), 
                habillage = datos$Credit_Score, 
                geom = "point", pointsize = 1, 
                xlim = c(-7,-2), ylim = c(-1.5,-0.5),
                select.ind = list(name = indices),
                addEllipses=TRUE, ellipse.level=0.95, 
                palette = c("indianred3","palegreen3","skyblue2"),
                col.var = "black",
                select.var = list(name = c("Interest_Rate",
                                           "Num_of_Loan",
                                           "Payment_of_Min_Amount",
                                           "Outstanding_Debt",
                                           "Num_Credit_Inquiries",
                                           "Num_Bank_Accounts",
                                           "Num_of_Delayed_Payment",
                                           "Delay_from_due_date",
                                           "Num_Credit_Card",
                                           "Changed_Credit_Limit")))
```
En esta tabla se muestra más detallado y en orden descendente a las variables de acuerdo a su correlación con el componente principal 1 y el grupo al que se relacionan.

```{r variables significativas, include = TRUE}
#Valor primer componente de las variables más significativas del grupo Good
Good_PCA_var <- as.data.frame(sort(pca$rotation[pca$rotation[,1] > 0.1,1], 
                   decreasing=TRUE))
colnames(Good_PCA_var) <- c("PCA1")
Good_PCA_var$Grupo <- "Good"

#Valor primer componente de las variables más significativas del grupo Standard
Standard_PCA_var <- as.data.frame(sort(pca$rotation[pca$rotation[,1] > -0.15 
                                                  & pca$rotation[,1] < 0.1,1], 
                    decreasing=TRUE))
colnames(Standard_PCA_var) <- c("PCA1")
Standard_PCA_var$Grupo <- "Standard"


#Valor primer componente de las variables más significativas del grupo Poor
Poor_PCA_var <- as.data.frame(sort(pca$rotation[pca$rotation[,1] < -0.15,1],
                     decreasing=TRUE))
colnames(Poor_PCA_var) <- c("PCA1")
Poor_PCA_var$Grupo <- "Poor"


rbind(Good_PCA_var,Standard_PCA_var,Poor_PCA_var) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```

Similar a lo visto en la sección de estadística descriptiva, las variables Age y Credit_History_Age muestran como aquellos con puntaje Good suelen ser quienes tienen más edad, y por tanto, un historial más antiguo. Monthly_Balance, Annual_Income, Monthly_Inhand_Salary, y Amount_invested_monthly muestran que aquellos con un mayor capital tanto en sus ingresos, sus cuentas de bancos y el dinero que invierten son los que suelen ser considerados también con una puntuación alta. Por tanto, este grupo se destaca por tener mayor antigüedad y una estabilidad financiera elevada.

Por otro lado, aquellos del grupo Poor están sumamente relacionados con las variables que implican muchos créditos y préstamos (Num_Credit_Card, Changed_Credit_Limit, Num_Credit_Inquiries, Num_of_Loan, Num_Bank_Accounts), deudas altas (Outstanding_Debt, Interest_Rate) y pagos atrasados (Delay_from_due_date, Num_of_Delayed_Payment, Payment_of_Min_Amount). Así, este grupo es identificable cuando se tienen muchos préstamos pedidos al banco, una deuda alta, muchas tarjetas de crédito y cuentas de banco, así como muchos pagos atrasados y muchos días de retraso, tasas de interés altas y muchos pagos mínimos realizados, indicando que el cliente no es capaz de cubrir sus deudas en tiempo y forma.

El grupo Standard solo tiene relacionadas las variables Credit_Utilization_Ratio y Total_EMI_per_month. En particular, estas variables tienen poca variabilidad por lo que es difícil identificar algún tipo de correlación entre ellas con el Credit Score. Sin embargo, el hecho de que las observaciones de este grupo se ubiquen en una posición intermedia entre las variables más correlacionadas con los otros 2 puntajes, precisamente muestra que aquellos que pertenecen a este grupo no tienen una antigüedad muy alta o estabilidad financiera elevada, pero tampoco muestran un comportamiento deudor elevado respecto a los créditos y préstamos que solicitan así como la manera en que pagan sus deudas.

Adicionalmente al PCA solo con estandarización de las variables, se realizó un PCA añadiendo rotación varimax y usando la matriz de correlación. 

```{r pca rotado, include = TRUE, fig.height=3, fig.width=6, fig.align='center'}
#pca con rotacion
pca_rot <- principal(datos[,-c(20:49)], nfactors = 10, 
                     rotate = "varimax", scores = TRUE, cor = TRUE)

#barchart de varianza explicada por componente
x = c(1:10)
y = pca_rot$Vaccounted[2,]
exp_var = data.frame("Dimension" = x,
                     "Percentage_of_explained_variance" = y)

ggplot(data=exp_var,aes(x = Dimension,y = Percentage_of_explained_variance)) +
  geom_col(fill="dodgerblue3",alpha=.85) + 
  geom_line(color = "black", size = 0.2) +
  geom_point(color = "black", size = 1) +
  scale_x_continuous(breaks = 1:10) +
  ggtitle("Scree plot") +
  theme_minimal()
```
En este caso la varianza en los 2 primeros componentes es alrededor del 37%, necesitamos al menos 3 componentes para alcanzar la varianza explicada similar a la que se tenía en 2 componentes con el PCA simple. Sin embargo, al graficar el biplot se observa que hay realmente un comportamiento similar entre ambos PCA's (sin y con rotación).

```{r pca rotado biplot, include = TRUE, fig.height=3.5, fig.width=7, fig.align='center'}
#pca con rotacion (2 componentes)
pca_rot <- principal(datos[,-c(20:49)], nfactors = 2, 
                     rotate = "varimax", scores = TRUE, cor = TRUE)

#biplot
credit_score_pca_rot <- data.frame(x=pca_rot$scores[indices,1],
                                   y=pca_rot$scores[indices,2],
                       Credit_Score=as.factor(datos[indices,'Credit_Score']))

ggplot(data=credit_score_pca_rot,aes(x=x,y=y,col=Credit_Score)) +
  geom_point(show.legend=TRUE) +
  theme_minimal() +
  xlim(-2,2.5) +
  ylim(-2,5) +
  labs(x="PC1",y="PC2", title = "PCA - Credit Score")
```

En este caso nótese que en efecto hubo una rotación de 180 grados y ahora la relación entre el PC1 y Credit Score es inversa (valores negativos de PC1 para el puntaje Good y valores positivos para el puntaje Poor).

En general, el análisis de componentes principales logra encontrar un distinción considerable entre los grupos Good y Poor, pero al considerar al grupo Standard la distinción entre las 3 clases se vuelve un poco más compleja visualmente hablando, sin embargo, la relación encontrada entre las variables de la base de datos y los grupos es de especial utilidad para saber como identificar a los 3 grupos, pero sobretodo a los grupos Poor y Good. El principal problema que podría surgir en la clasificación es respecto a clasificar a los del grupo Poor como Good, pues nótese que gráficamente muchos del grupo Poor en ambos PCA's se encuentran en la región del grupo Good, pero pocas observaciones del grupo Good están en el grupo Poor.

## K-Means

Se realizó una agrupación con K-Means con K = 3. Se estandarizaron los datos y se utilizaron hasta 100 conjuntos aleatorios de centros iniciales y 1000 iteraciones permitidas por cada grupo de centros iniciales. Nota: Se pobraron distintos valores de nstart e iter.max, pero se eligieron 100 y 1000 respectivamente ya que con otros valores se llegaba a resultados similares o peores a los presentados.

```{r kmeans, include = TRUE, fig.height=3.5, fig.width=7, fig.align='center'}
set.seed(2)
datos_scaled <- datos
datos_scaled[,1:19] <- scale(datos[,1:19]) #estandarización variables numéricas
k_means <- kmeans(datos_scaled[,-c(20:49)], 
                  iter.max = 1000, centers = 3, 
                  nstart = 100)

#biplot agrupado por las clases de k-means
k_means_pca <- data.frame(x=pca$x[indices,1],
                          y=pca$x[indices,2],
                       Cluster=as.factor(k_means$cluster[indices]))


ggplot(data=k_means_pca,aes(x=x,y=y,col=Cluster)) +
  geom_point(show.legend=TRUE) +
  theme_minimal() +
  xlim(-7,7) +
  ylim(-6,3) +
  labs(x="PC1",y="PC2", title = "PCA - K-means / k = 3") +
  scale_color_manual(values = c("1" = "skyblue2", "2" = "indianred3", 
                                "3" = "palegreen3"))

#biplot con rotación agrupado por las clases de k-means
k_means_pca_rot <- data.frame(x=pca_rot$scores[indices,1],
                              y=pca_rot$scores[indices,2],
                       Cluster=as.factor(k_means$cluster[indices]))

ggplot(data=k_means_pca_rot,aes(x=x,y=y,col=Cluster)) +
  geom_point(show.legend=TRUE) +
  theme_minimal() +
  xlim(-2,2.5) +
  ylim(-2,5) +
  labs(x="PC1",y="PC2", title = "PCA con rotación - K-means / k = 3") +
  scale_color_manual(values = c("1" = "skyblue2", "2" = "indianred3", 
                                "3" = "palegreen3"))
```
En ambos gráficos se observa que el cluster 2 se ubica en una región bastante similar a la que se ubicaban los del grupo Poor, tanto en la zona negativa del componente principal 1 en el caso del PCA simple con estandarización, como en la zona positiva del componente principal 1 en el caso del PCA usando matriz de correlación y con rotación varimax. Sin embargo, los clusters 1 y 3 se dividen aproximadamente de acuerdo al componente principal 2 y no al componente prinicipal 1 como se había observado en PCA. Entonces, K-Means logra hacer una distinción entre Poor con respecto a Good y Standard, lo cual en sentido estricto es bueno ya que este grupo es principalmente al que se busca clasificar, pero falla si queremos también hacer una distinción ideal con los del grupo Good, el cual también es un grupo importante.

## Método Jerárquico

Para este método, se usó la distancia euclideana y el método Ward. Dado que una matriz de distancias 100 mil observaciones es muy costosa computacionalmente, se utilizó una muestra significativa de 10 mil observaciones para evaluar el método jerárquico, respetando las proporciones de Credit Score (53% Standard, 29% Poor y 18% Good). Nota: Al igual que K-means, se probaron distintas métricas como manhattan, maximum, canberra, minkowski y binary, así como varios métodos como complete, single y average. Lo que más influyó fue el método, pues single y average arrojaban clusters muy desproporcionados, mientras que complete y ward arrojaron resultados similares para casi todas las métricas, de la cual la euclideana fue la que mostró resultados más distinguibles.

```{r metodo jerarquico dendrograma, include = TRUE, fig.align='center'}
#ajustar el metodo jerarquico 
set.seed(3)
indices_mj <- c(sample(which(datos$Credit_Score == "Good"), 1800), 
                          sample(which(datos$Credit_Score == "Standard"), 5300), 
                          sample(which(datos$Credit_Score == "Poor"), 2900))
datos_sample <- datos_scaled[indices_mj, -c(20:49)]
metodo_jerarquico <- hclust(dist(datos_sample, 
                                 method = "euclidean"), 
                            method="ward.D")
plot(metodo_jerarquico) #dendrograma
rect.hclust(metodo_jerarquico, k = 3, border=2:6)
```

El dendograma alrededor de la altura 3000 muestra 3 clusters bastante distinguibles, de los cuales por la longitud en el eje x o de las observaciones de cada cluster, podemos deducir por las proporciones antes mencionadas que se tratan de los grupos Good (izquierda/18%), Standard (centro/53%) y Poor (derecha/29%). A diferencia de K-Means, el cuál no distinguía entre el grupo Good y Standard, el método jerárquico a altura 4000 aproximadamente encuentra que los grupos Standard y Poor tienen más similtud que el grupo Good.

```{r metodo jerarquico, include = TRUE, fig.height=3.5, fig.width=7, fig.align='center'}
#biplot agrupado por las clases del método jerárquico
metodo_jerarquico_pca <- data.frame(x=pca$x[indices_mj,1],
                                    y=pca$x[indices_mj,2],
                       Cluster=as.factor(cutree(metodo_jerarquico, 
                                                k = 3)))

ggplot(data=metodo_jerarquico_pca,aes(x=x,y=y,col=Cluster)) +
  geom_point(show.legend=TRUE) +
  theme_minimal() +
  xlim(-7,7) +
  ylim(-6,3) +
  labs(x="PC1",y="PC2", title = "PCA - Metodo jerárquico") +
  scale_color_manual(values = c("1" = "palegreen2", "2" = "skyblue2", 
                                "3" = "indianred3"))

#biplot con rotación agrupado por las clases del método jerárquico
metodo_jerarquico_pca_rot <- data.frame(x=pca_rot$scores[indices_mj,1],
                                        y=pca_rot$scores[indices_mj,2],
                       Cluster=as.factor(cutree(metodo_jerarquico, k = 3)))

ggplot(data=metodo_jerarquico_pca_rot,aes(x=x,y=y,col=Cluster)) +
  geom_point(show.legend=TRUE) +
  theme_minimal() +
  xlim(-2,2.5) +
  ylim(-2,5) +
  labs(x="PC1",y="PC2", title = "PCA con rotación - Metodo jerárquico") +
  scale_color_manual(values = c("1" = "palegreen2", "2" = "skyblue2", 
                                "3" = "indianred3"))
```
Observando los clusters mediante componentes principales, a diferencia de K-Means, se obtienen clusters muy similares a los visualizados en PCA, pues nótese que hay una separación a lo largo del componente principal 1 para los 3 clusters obtenidos (valores negativos, cercanos a 0, y positivos). 

Por lo tanto, con este método hay evidencia de que si es posible encontrar separaciones algo generales entre los 3 grupos, y sobretodo, junto con lo que arrojó K-Means, es posible encontrar un diferenciador sobre los grupos Good y Poor. Sin embargo, es importante aclarar que a pesar de que ambos métodos lograron identificar al grupo Poor de manera similar, en PCA se mostró que algunas observaciones de este grupo podrían terminan siendo dificiles de clasificar correctamente debido a que comparten valores similares con la mayoría de las observaciones del grupo Good, además de que el grupo Standard se fuga entre ambos grupos de igual manera, por lo que los métodos para predecir el Credit Score podrían no tener un Accuracy muy alto debido a este porcentaje significativo de observaciones que podrían considerarse como "outliers" respecto al grupo que corresponden.

# Predicción de Credit Score

En esta sección se muestran diferentes modelos utilizados para intentar predecir el Credit Score. Para evaluar el poder predictivo general de cada uno de los modelos, se usó un K Cross Validation con K = 5, de manera que se tienen 5 pliegues de 20 mil observaciones cada uno, donde en cada una de las 5 evaluaciones del poder predictivo se consideran 4 pliegues para entrenar los modelos (80 mil observaciones de train) y 1 pliegue para comparar las predicciones hechas por cada modelo respecto a las reales (20 mil observaciones de test). 

Las métricas que se consideran son la Tasa de Clasificación Global (Accuracy), Sensibilidad (Recall), Especificidad (Specificity), F1-Score (F1) y el Área bajo la Curva ROC (ROC-AUC). Dado que el problema de clasificación involucra 3 clases, se calcularon estas métricas por clase considerando un problema binario para cada clase (Poor vs. Standard & Good, Standard vs. Poor & Good y Good vs. Standard & Poor). Para obtener el rendimiento general del modelo por métrica, se tomó el promedio de lo obtenido por métrica en las 3 clases. En cada sección, se muestran 2 tablas, la primera tabla con las métricas generales del modelo (promedio de cada métrica en las 3 clases), y la segunda tabla con el desglose de las métricas por clase considerando los problemas binarios mencionados.

En la mayoría de los modelos presentados es necesario hacer un tuneo de hiperparámetros para encontrar el hiperparámetro o la combinación de hiperparámetros más óptima en cada una de las iteraciones del Cross Validation. Por lo tanto, para cada iteración, usando solo datos del train, se vuelve a realizar un K Cross Validation con K = 3 para evaluar el poder predictivo por cada combinación de hiperparámetros. La métrica usada para tomar la decisión de que hiperparámetros son los mejores es el Accuracy.

En particular, con todo lo analizado en las secciones pasadas, además de querer clasificar correctamente a cada una de las clases, queremos que aquellos que sean del grupo Poor si sean clasificados en ese grupo, además de que si el modelo predice a alguien en el grupo Good, la probabilidad de que realmente sea del grupo Good sea alta, ya que como banco queremos identificar quienes son los que cuentan con un mayor riesgo de no pagar sus créditos y préstamos, así como evitar dar préstamos a clientes que muy probablemente no pagarán sólo porque el modelo los clasificó en una clase buena. Por lo tanto, las métricas más importantes en el análisis del poder predictivo serán el Accuracy, Recall del grupo Poor y Precision del grupo Good.

```{r Folds K Cross Validation, include = TRUE}
#ordenamos los niveles 
datos$Credit_Score <- ordered(datos$Credit_Score, levels = c("Poor", "Standard", "Good"))

#Partición 5-Cross-Validation
set.seed(4)
K = 5
cv_etiquetas = rep(1:K, length.out = dim(datos)[1])
Folds <- sample(cv_etiquetas) #asigna a cada obs a uno de los K pliegues
```

```{r paralelización, include = TRUE}
num_cores <- detectCores() - 1
cl <- makeCluster(num_cores)  
registerDoParallel(cl)
#stopCluster(cl)
#registerDoSEQ()
```

## Regresión Logística Multinomial

Para este modelo, se consideró obtener los coeficientes usando penalización Lasso, teniendo como el modelo más grande el que incluía a todas las variables más todas las variables numéricas al cuadrado, por lo que se debe tunear el parámetro $\lambda$ de la penalización. Para cada iteración del Cross Validation, se consideró una malla de 50 valores distintos de $\lambda$ y se usó la función glmnet para el tuneo.

```{r poder predictivo regresion logistica multinomial, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando regresión logística multinomial con penalización lasso mediante K Cross Validation
reg.log.mult.PP = function(k, Folds, datos, formula){
  #Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  X.train.cv <- model.matrix(formula, 
                        data = datos[train.cv.k_indices,])[,-1] #matriz modelo
  Y.train.cv <- datos[train.cv.k_indices,"Credit_Score"] #variable Y del train
  reg.log.mult.lasso.tun <- cv.glmnet(X.train.cv, Y.train.cv, #tuneo de lambda
                                       nfolds = 5, type.measure = "class", 
                                       gamma = 0, relax = FALSE, 
                                       family = "multinomial", nlambda = 50) 
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del plieque k
  X.test.cv <- model.matrix(formula, 
                        data = datos[test.cv.k_indices,])[,-1] #matriz modelo
  Y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #variable Y del train
  pred.class <- predict(reg.log.mult.lasso.tun, #predicciones (clases)
                        newx = X.test.cv, 
                        type = "class", s = "lambda.min") 
  pred.prob <- predict(reg.log.mult.lasso.tun, #predicciones (probabilidades)
                       newx = X.test.cv, 
                       type = "response", s = "lambda.min") 
  pred.prob <- as.data.frame(pred.prob[,,1])
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = Y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(Y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(Y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(Y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,"Poor"],
                pred.Standard = pred.prob[,"Standard"],
                pred.Good = pred.prob[,"Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = Y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```


```{r k cross validation regresion logistica multinomial, eval = FALSE}
tic()
#formula considerando todas las variables y las numéricas al cuadrado
formula = as.formula(paste('Credit_Score ~.',"+", 
                           paste('I(',names(datos[,-c(20:49)]),'^2)',
                                 collapse = ' + '))) 

#Evaluar el poder predictivo en cada uno de los K pliegues
reg.log.mult.CV <- foreach(K = 1:5, 
                  .combine = c, 
                  .packages = c("glmnet", "yardstick", "caret")) %dopar% 
                  {
                  write(paste(Sys.time(), 
                        "Regresión Logística Multinomial Procesando fold K =", K), 
                        file = "Progreso.txt", #registrar inicio de cada pliegue K
                        append = TRUE) 
                  reg.log.mult.PP(K, Folds = Folds, #ejecutar la función para cada pliegue
                                  datos = datos[,-c(20,35,43)], 
                                  formula = formula) 
                  }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(reg.log.mult.CV[[1]], 
                           reg.log.mult.CV[[3]], 
                           reg.log.mult.CV[[5]], 
                           reg.log.mult.CV[[7]], 
                           reg.log.mult.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(reg.log.mult.CV[[2]], 
                       reg.log.mult.CV[[4]], 
                       reg.log.mult.CV[[6]], 
                       reg.log.mult.CV[[8]], 
                       reg.log.mult.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.reg.log.mult <- Reduce("+", metricas.generales) / n.generales
metricas.clase.reg.log.mult <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.reg.log.mult,"metricas.generales.reg.log.mult.rds")
saveRDS(metricas.clase.reg.log.mult,"metricas.clase.reg.log.mult.rds")
```

```{r resultados regresion logistica multinomial, include = TRUE}
metricas.generales.reg.log.mult <- readRDS("metricas.generales.reg.log.mult.rds")
metricas.clase.reg.log.mult <- readRDS("metricas.clase.reg.log.mult.rds")

t(metricas.generales.reg.log.mult) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
metricas.clase.reg.log.mult %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```
El accuracy obtenido es del 66%, el cual es un valor que definitivamente puede mejorar, ya que solo 2 de 3 clientes están siendo clasificados en la clase correcta. De Recall general se obtiene un valor similar al accuracy de 64% pero notemos que precisamente la clase Poor que es la que más nos interesa clasificar correctamente no está teniendo un buen rendimiento, pues solo el 52.75% se predicen realmente como clase Poor, y lo mismo ocurre con los que son clasificados en la clase Good, en la Precision vemos que solo el 54.17% son realmente de esa clase, lo que nos indica entonces que muchos de la clase Poor probablemente están siendo clasificados como Good. En la clase Standard el rendimiento es mejor tanto en Recall como en Precision, con valores de 73% y 70% respectivamente, y por lo mismo Specificity para las clases Poor y Good es bastante alto, alrededor del 87%-89% debido a la alta clasificación de la clase Standard, pero justo en esta clase su Specificity es solo del 65%, similar al accuracy general, ya que las clases Poor y Good no son correctamente clasificadas.

En general, este modelo tiene un accuracy decente pero que se puede mejorar, pero precisamente las métricas de interés más particulares como lo son el Recall de la clase Poor y la Precision de la clase Good son muy bajas, mientras que la clasificación de la clase Standard es la mejor de las 3, por lo que podemos concluir que la regresión logística multinomial no está capturando adecuadamente las relaciones entre las clases y el resto de variables, que estás relaciones no son necesariamente lineales sino más complejas y por lo mismo generaliza mucho las características de las 3 clases, donde solo clasifica correctamente a aquellos que presenten valores en las variables muy distintivas a la clase que realmente pertenecen como se observo en la sección anterior.

## LDA

En este modelo no hubo ningún hiperparámetro para tunear, por lo que su entrenamieto y evaluación fue directo.

```{r poder predictivo LDA, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando LDA mediante K Cross Validation
LDA.PP = function(k, Folds, datos, formula){
  #Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  LDA.model <- lda(formula, data = datos[train.cv.k_indices,])
  
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del pliegue k
  pred <- predict(LDA.model, newdata = datos[test.cv.k_indices,]) #predicciones
  pred.class <- pred$class #predicciones (clases)
  pred.prob <- pred$posterior #predicciones (probabilidades)
  pred.prob <- as.data.frame(pred.prob)
  y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #valores reales
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,"Poor"],
                pred.Standard = pred.prob[,"Standard"],
                pred.Good = pred.prob[,"Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```


```{r k cross validation LDA, eval = FALSE}
tic()
#formula considerando todas las variables
formula = as.formula("Credit_Score ~ .")

#Evaluar el poder predictivo en cada uno de los K pliegues
LDA.CV <- foreach(K = 1:5, 
                  .combine = c, 
                  .packages = c("MASS", "yardstick", "caret")) %dopar% 
          {
          write(paste(Sys.time(), 
                "LDA Procesando fold K =", K), 
                file = "Progreso.txt", #registrar inicio de cada pliegue K
                append = TRUE) 
          LDA.PP(K, Folds = Folds, #ejecutar la función ára cada pliegue
                 datos=datos[,-c(20,35,43)], 
                 formula=formula) 
          }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(LDA.CV[[1]], 
                           LDA.CV[[3]], 
                           LDA.CV[[5]], 
                           LDA.CV[[7]], 
                           LDA.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(LDA.CV[[2]], 
                       LDA.CV[[4]], 
                       LDA.CV[[6]], 
                       LDA.CV[[8]], 
                       LDA.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.LDA <- Reduce("+", metricas.generales) / n.generales
metricas.clase.LDA <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.LDA,"metricas.generales.LDA.rds")
saveRDS(metricas.clase.LDA,"metricas.clase.LDA.rds")
```


```{r resultados LDA, include = TRUE}
metricas.generales.LDA <- readRDS("metricas.generales.LDA.rds")
metricas.clase.LDA <- readRDS("metricas.clase.LDA.rds")

t(metricas.generales.LDA) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
metricas.clase.LDA %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```
Tenemos resultados muy similares a la regresión logística multinomial, ya que el accuracy es cercano al 66%, el Recall de la clase Poor aumentó a 55% pero la Precision de la clase Good disminuyó a 52%. Volvemos a observar que la Specificity de las clases Good y Poor es alta debido a que la clase Standard tiene un Recall y Precision alrededor del 70% (F1 Score de 71%) con respecto al Recall y Precision de los otros 2 grupos (F1 Score de 60% aproximadamente). Por tanto, este modelo ni mejoró ni empeoró las métricas del modelo anterior, solamente la ventaja es que a diferencia de la regresión logística multinomial, LDA tiene un entrenamiento mucho más rápido, por lo que obtenemos resultados similares con un menor costo computacional, pero como ya se explicó, estas métricas deben ser mejoradas.

## QDA

Al igual que LDA, este modelo no tiene hiperarámetros a tunear, por lo que también su entrenamiento y evaluación fue directo, aunque al ser un modelo más complejo que LDA tomó más tiempo de entrenamiento y evaluación pero no tanto como la regresión logística multinomial.

```{r poder predictivo QDA, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando QDA mediante K Cross Validation
QDA.PP = function(k, Folds, datos, formula){
  #Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  QDA.model <- qda(formula, data = datos[train.cv.k_indices,])
  
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del pliegue k
  pred <- predict(QDA.model, newdata = datos[test.cv.k_indices,]) #predicciones
  pred.class <- pred$class #predicciones (clases)
  pred.prob <- pred$posterior #predicciones (probabilidades)
  pred.prob <- as.data.frame(pred.prob)
  y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #valores reales
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,"Poor"],
                pred.Standard = pred.prob[,"Standard"],
                pred.Good = pred.prob[,"Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```

```{r k cross validation QDA, eval = FALSE}
tic()
#formula considerando todas las variables
formula = as.formula("Credit_Score ~ .")

#Evaluar el poder predictivo en cada uno de los K pliegues
QDA.CV <- foreach(K = 1:5, 
                  .combine = c, 
                  .packages = c("MASS", "yardstick", "caret")) %dopar% 
          {
          write(paste(Sys.time(), 
                "QDA Procesando fold K =", K), 
                file = "Progreso.txt", #registrar inicio de cada pliegue K
                append = TRUE) 
          QDA.PP(K, Folds = Folds, #ejecutar la función ára cada pliegue
                 datos=datos[,-c(20,35,43)], 
                 formula=formula) 
          }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(QDA.CV[[1]], 
                           QDA.CV[[3]], 
                           QDA.CV[[5]], 
                           QDA.CV[[7]], 
                           QDA.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(QDA.CV[[2]], 
                       QDA.CV[[4]], 
                       QDA.CV[[6]], 
                       QDA.CV[[8]], 
                       QDA.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.QDA <- Reduce("+", metricas.generales) / n.generales
metricas.clase.QDA <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.QDA,"metricas.generales.QDA.rds")
saveRDS(metricas.clase.QDA,"metricas.clase.QDA.rds")
```


```{r resultados QDA, include = TRUE}
metricas.generales.QDA <- readRDS("metricas.generales.QDA.rds")
metricas.clase.QDA <- readRDS("metricas.clase.QDA.rds")

t(metricas.generales.QDA) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
metricas.clase.QDA %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```
El accuracy obtenido fue de 65% aproximadamente, disminuyó ligeramente alrededor del 1% respecto a la regresión logística multinomial y LDA, sin embargo, el Recall de la clase Poor si aumentó considerablemente de 52%-55% a 75%, pero la Precision de la clase Good se mantuvo igual alrededor del 50%, y además, notemos que en este caso la clase Standard disminuyó mucho en su Recall, pasando de 70%-73% a solo 54%. Además, el Recall de la clase Good en los otros 2 modelos tenía un valor de 70% pero en este caso aumentó en 80%. Por tanto, QDA está logrando distinguir a las clases Good y Poor de una mejor manera que la regresión logística multinomial y LDA, pero esta separación muy seguramente está incluyendo a todas esas observaciones del grupo Standard que visulamente cuando se observó el biplot del PCA estaban mezcladas en ambos grupos Good y Poor. Por tanto, si tenemos una métrica mejorada que es el Recall de la clase Poor, pero el Accuracy y la Precision del grupo Good siguen sin mejorar, por lo que es necesario probar otros modelos.

## Naive Classifier

Para este modelo se tuneó el hiperparámetro de laplace $k$, el cual corrige la probabilidad para evitar tener probabilidades de 0 debido a alguna variable. Se consideraron los valores de $k = 1,5,10,20,50$, y el tuneo se realizó con la paquetería caret. Además, solo se trabajo con las 19 variables numéricas debido a que al tener muchas variables dummies (29 en total), esto podría ocasionar que incluso teniendo probabilidades condicionales altas por variable, al tener tantas variables la probabilidad fuese muy cercana a cero (ejemplo $0.9^{48} = 0.006$), lo cual no es ideal considerando que las 29 dummies solo representan la información de otras 3 variables categóricas más.

```{r poder predictivo Naive Classifier, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando Naive Classifier mediante K Cross Validation
Naive.PP = function(k, Folds, datos, formula){
  #Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  Naive.model.tun <- train(formula, #Naive classifier con tuneo de hiperparámetros
                       data = datos[train.cv.k_indices,],
                       method = "nb",
                       trControl = trainControl(method = "cv", number = 3),
                       tuneGrid = expand.grid(fL = c(5,10,20,50), #malla de laplace
                                              usekernel = FALSE,
                                              adjust = 1))
  
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del pliegue k
  pred.class <- predict(Naive.model.tun, 
                        newdata = datos[test.cv.k_indices,]) #predicciones (clases)
  pred.prob <- predict(Naive.model.tun, 
                       newdata = datos[test.cv.k_indices,], 
                       type = "prob") #predicciones (probabilidades)
  pred.prob <- as.data.frame(pred.prob)
  y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #valores reales
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,"Poor"],
                pred.Standard = pred.prob[,"Standard"],
                pred.Good = pred.prob[,"Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```


```{r k cross validation Naive Classifier, eval = FALSE}
tic()
#formula considerando todas las variables
formula = as.formula("Credit_Score ~ .")

#Evaluar el poder predictivo en cada uno de los K pliegues
Naive.CV <- foreach(K = 1:5, 
                  .combine = c, 
                  .packages = c("yardstick", "klaR", "caret")) %dopar% 
            {
            write(paste(Sys.time(), 
                  "Naive Procesando fold K =", K), 
                  file = "Progreso.txt", #registrar inicio de cada pliegue K
                  append = TRUE) 
            Naive.PP(K, Folds = Folds, #ejecutar la función ára cada pliegue
                   datos=datos[,-c(20:48)], 
                   formula=formula) 
            }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(Naive.CV[[1]], 
                           Naive.CV[[3]], 
                           Naive.CV[[5]], 
                           Naive.CV[[7]], 
                           Naive.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(Naive.CV[[2]], 
                       Naive.CV[[4]], 
                       Naive.CV[[6]], 
                       Naive.CV[[8]], 
                       Naive.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.Naive <- Reduce("+", metricas.generales) / n.generales
metricas.clase.Naive <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.Naive,"metricas.generales.Naive.rds")
saveRDS(metricas.clase.Naive,"metricas.clase.Naive.rds")
```


```{r resultados Naive, include = TRUE}
metricas.generales.Naive <- readRDS("metricas.generales.Naive.rds")
metricas.clase.Naive <- readRDS("metricas.clase.Naive.rds")

t(metricas.generales.Naive) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
metricas.clase.Naive %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```

Este por el momento es el peor modelo que se tiene en términos del accuracy, pues solo el 61.66% de las observaciones se clasificaron correctamente. Además, obtenemos resultados similares a los obtenidos con QDA pero con métricas incluso peores, el Recall de la clase Poor bajo ligeramente a 73%, pero la Precision del grupo Good disminuyó a 43%, y la clase Standard nuevamente tiene un Recall muy bajo del 47%, y el Recall de la clase Good aumentó a 84%, indicando entonces que este modelo hace una separación todavía más drástica entre las clases Good y Poor que QDA, sin lograr distinguir a aquellos de la clase Standard que se encuentran mezclados en los otros 2 grupos. 

## K Nearests Neighbors

En este modelo el hiperparámetro tuneado fue $k$, de cuál se consideró una malla con los valores $k = 3,5,7$. Al igual que con Naive, se usó caret para el tuneo y se consideraron solamente las variables numéricas, en este caso debido a que al tener más dummies que variables numéricas, estas podrían predominar considerablemente en el cálculo de las distancias entre observaciones.

```{r poder predictivo KNN, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando K-Nearest-Neighbors mediante K Cross Validation
KNN.PP = function(k, Folds, datos, formula){
  #Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  KNN.model.tun <- train(formula, #KNN con tuneo de hiperparámetros
                       data = datos[train.cv.k_indices,],
                       method = "knn",
                       preProcess = c("center","scale"), #estandarización
                       trControl = trainControl(method = "cv", 
                                                number = 3),
                       tuneGrid = expand.grid(k = c(3,5,7))) #malla de k
  
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del pliegue k
  pred.class <- predict(KNN.model.tun, 
                        newdata = datos[test.cv.k_indices,]) #predicciones (clases)
  pred.prob <- predict(KNN.model.tun, 
                       newdata = datos[test.cv.k_indices,], 
                       type = "prob") #predicciones (probabilidades)
  pred.prob <- as.data.frame(pred.prob)
  y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #valores reales
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,"Poor"],
                pred.Standard = pred.prob[,"Standard"],
                pred.Good = pred.prob[,"Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```

```{r k cross validation KNN, eval = FALSE}
tic()
#formula considerando todas las variables
formula = as.formula("Credit_Score ~ .")

#Evaluar el poder predictivo en cada uno de los K pliegues
KNN.CV <- foreach(K = 1:5, 
                  .combine = c, 
                  .packages = c("yardstick", "caret")) %dopar% 
                  {
                  write(paste(Sys.time(), 
                        "KNN Procesando fold K =", K), 
                        file = "Progreso.txt", #registrar inicio de cada pliegue K
                        append = TRUE) 
                  KNN.PP(K, Folds = Folds, #ejecutar la función ára cada pliegue
                         datos=datos[,-c(20:48)], 
                         formula=formula) 
                  }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(KNN.CV[[1]], 
                           KNN.CV[[3]], 
                           KNN.CV[[5]], 
                           KNN.CV[[7]], 
                           KNN.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(KNN.CV[[2]], 
                       KNN.CV[[4]], 
                       KNN.CV[[6]], 
                       KNN.CV[[8]], 
                       KNN.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.KNN <- Reduce("+", metricas.generales) / n.generales
metricas.clase.KNN <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.KNN,"metricas.generales.KNN.rds")
saveRDS(metricas.clase.KNN,"metricas.clase.KNN.rds")
```

```{r resultados KNN, include = TRUE}
metricas.generales.KNN <- readRDS("metricas.generales.KNN.rds")
metricas.clase.KNN <- readRDS("metricas.clase.KNN.rds")

t(metricas.generales.KNN) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
metricas.clase.KNN %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```
A diferencia del resto de modelos, KNN logra aumentar considerablemente todas las métricas de interés, principalmente el Accuracy, donde tenemos un 75.78% de clasificación correcta a diferencia del 61%-66% que los otros modelos estaban obteniendo, el Recall de la clase Poor aumenta ligeramente a 77% pero la Precision del grupo Good si aumenta considerablemente a 68.5%. En este caso, las 3 clases están obteniendo valores más equilibrados tanto de Recall como de Precision (68% a 77%), indicandonos que si hay una mejora tanto en la predicción que están obteniendo las observaciones de una clase verdadera, como en a que clase realmente pertenecen las predicciones que hace el modelo de una clase en específico. En este caso, el grupo Good es quien tiene el Recall y Precision más bajo (F1 Score del 68.5%), ya que como se observó en el biplot al hacer PCA, en la región de las observaciones del grupo Good también se mezclaban muchas observaciones de los grupos Standard y Poor, lo cual no pasaba es con el grupo Poor, el cual solamente tenía en su mayoría observaciones Standard mezcladas en su región. Pero en general, KNN logra aumentar y equilibrar las métricas de interés por clase.

## Árbol de Decisión

Para este modelo, tenemos 3 hiperparámetros a tunear, los cuales son la complejidad del arbol (cost_complexity), la profundidad del árbol (tree_depth), y el número mínimo de observaciones que deben tener los nodos finales (min_n).
Para cada uno, se consideraron las mallas por defecto de cada hiperparámetro dadas por tidymodels:

- *cost_complexity*: $[10^{-10},0.01]$
- *tree_depth*: $[1,30]$
- *min_n*: $[2,40]$

Sin embargo, no se realizó un tuneo completo considerando todas las posibles combinaciones de hiperparámetros en esos intervalos, sino que se recurrió a una busqueda aleatoria, tomando aleatoriamente solo 20 combinaciones de hiperparámetros entre esos rangos.

```{r poder predictivo Arbol de decision, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando Arbol de Decisión mediante K Cross Validation
Arbol.Decision.PP = function(k, Folds, datos, formula){
  #Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  
  #Modelo con hiperparámetros a tunear
  Arbol.Decision.modelo <- decision_tree(
    cost_complexity = tune(), #complejidad del árbol
    tree_depth = tune(), #profundidad del árbol
    min_n = tune()  #numero minimo de observaciones por nodo final
  ) %>%
    set_engine("rpart") %>%
    set_mode("classification")
  
  #Receta (fórmula)
  Arbol.Decision.receta <- recipe(formula, 
                                  data = datos[train.cv.k_indices,])
  
  #Workflow (modelo y fórmula)
  Arbol.Decision.workflow <- workflow() %>%
    add_model(Arbol.Decision.modelo) %>%
    add_recipe(Arbol.Decision.receta)
  
  #Grid de hiperparámetros
  grid.tun <- grid_random(
    parameters(Arbol.Decision.modelo),
    size = 20
  )
  
  #Pliegues a usar en el CV del tuneo
  Folds.tun <- vfold_cv(datos[train.cv.k_indices,], v = 3)
  
  #Tuneo de hiperparámetros
  Arbol.Decision.tun <- tune_grid(
    Arbol.Decision.workflow,
    resamples = Folds.tun,
    grid = grid.tun
  )
  
  #Obtener los mejores hiperparámetros
  best.params <- select_best(Arbol.Decision.tun, metric = "accuracy")
  
  #Obtener el mejor modelo
  Arbol.Decision.model.final <- finalize_workflow(Arbol.Decision.workflow, 
                                                best.params)
  
  #Entrenar el modelo final
  Arbol.Decision.model.tun <- fit(Arbol.Decision.model.final, 
                                  data = datos[train.cv.k_indices,])
  
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del pliegue k
  pred.class <- predict(Arbol.Decision.model.tun, 
                        new_data = datos[test.cv.k_indices,]) #predicciones (clases)
  pred.class <- as.vector(pred.class$.pred_class)
  pred.prob <- predict(Arbol.Decision.model.tun, 
                       new_data = datos[test.cv.k_indices,], 
                       type = "prob") #predicciones (probabilidades)
  pred.prob <- as.data.frame(pred.prob)
  y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #valores reales
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,".pred_Poor"],
                pred.Standard = pred.prob[,".pred_Standard"],
                pred.Good = pred.prob[,".pred_Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```


```{r k cross validation Arbol de decision, eval = FALSE}
tic()
set.seed(5)
#formula considerando todas las variables
formula = as.formula("Credit_Score ~ .")

Arbol.Decision.CV <- foreach(K = 1:5, 
                      .combine = c, 
                      .packages = c("parsnip","recipes","workflows","tune",
                                    "dials","rsample","yardstick","rpart",
                                    "caret")) %dopar% 
                      {
                      write(paste(Sys.time(), 
                            "Arbol Procesando fold K =", K), 
                            file = "Progreso.txt", #registrar inicio de cada pliegue K
                            append = TRUE) 
                      Arbol.Decision.PP(K, Folds = Folds, #ejecutar la función ára cada pliegue
                             datos=datos[,-c(20:48)], 
                             formula=formula) 
                      }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(Arbol.Decision.CV[[1]], 
                           Arbol.Decision.CV[[3]], 
                           Arbol.Decision.CV[[5]], 
                           Arbol.Decision.CV[[7]], 
                           Arbol.Decision.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(Arbol.Decision.CV[[2]], 
                       Arbol.Decision.CV[[4]], 
                       Arbol.Decision.CV[[6]], 
                       Arbol.Decision.CV[[8]], 
                       Arbol.Decision.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.Arbol.Decision <- Reduce("+", metricas.generales) / n.generales
metricas.clase.Arbol.Decision <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.Arbol.Decision,"metricas.generales.Arbol.Decision.rds")
saveRDS(metricas.clase.Arbol.Decision,"metricas.clase.Arbol.Decision.rds")
```

```{r resultados Arbol de decision, include = TRUE}
metricas.generales.Arbol.Decision <- readRDS("metricas.generales.Arbol.Decision.rds")
metricas.clase.Arbol.Decision <- readRDS("metricas.clase.Arbol.Decision.rds")

t(metricas.generales.Arbol.Decision) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
metricas.clase.Arbol.Decision %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```
A diferencia de KNN, tenemos que el rendimiento si disminuyó ligeramente, pues se obtiene un Accuracy del 73.44% respecto al 75.78% de KNN, el Recall tanto del grupo Poor como del grupo Standard baja de 77% en ambos en KNN a 73% y 74% respectivamente, pero sobretodo la Precision del grupo Good baja considerablmente de 68% en KNN a 60%. Esto nos indica que el Árbol de decisión tiene un poco más de dificultad en clasificar a las observaciones de las clases Poor y Standard que son similares a las del grupo Good, por lo mismo los Recall de Poor y Standard bajan así como la Precision de Good. Sin embargo, si consideramos más árboles y más variabilidad en los mismos así como el número de variables y la muestra que cada árbol considera, podríamos mejorar estas métricas, para detectar patrones más profundos.

## Bosque Aleatorio

Para los bosques aleatorios consideramos igualmente tunear el número mínimo de observaciones que deben tener los nodos finales (min_n), sin embargo, en lugar de modificar la complejidad de cada árbol y su profundidad máxima, estos hiperparámetros los sustituimos por la cantidad de árboles que tendrá el bosque (trees) y el número de variables que cada árbol considerará (mtry), de modo que las exactitud de las predicciones ya no dependa tanto de que tan complejos y profundos son los árboles, sino de que tanta variabilidad exista entre ellos. En este caso, se fijaron mallas por hiperparámetros manualmente con los siguientes valores:

- *mtry*: $[5,20]$
- *trees*: $[100,500]$
- *min_n*: $[10,100]$

Al igual que en el árbol de decisión, sólo se tomo aleatoriamente 20 combinaciones de estos hiperparámetros para reducir el costo computacional. Además, dado que se considerarán solo de 5 a 20 variables por árbol, se decidió entrenar solo usando las 19 variables numéricas, para evitar tener muchos árboles con solo variables dummies, que al ser 29, podrían estar muy presentes en todos lo árboles y por tanto, reducir la variabilidad de los mismos (por ejemplo, un árbol con 15 variables pero de las cuáles 8 sean dummies de una categórica, 5 de otra, y solo 2 numéricas, teniendo en realidad un árbol de 4 variables en lugar de 15).

```{r poder predictivo Bosque Aleatorio, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando Bosque Aleatorio mediante K Cross Validation
Bosque.Aleatorio.PP = function(k, Folds, datos, formula){
  #Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  
  #Modelo con hiperparámetros a tunear
  Bosque.Aleatorio.modelo <- rand_forest(
    mtry = tune(), #variables por árbol
    trees = tune(), #numero de arboles
    min_n = tune()  #numero minimo de observaciones por nodo final
  ) %>%
    set_engine("ranger") %>%
    set_mode("classification")
  
  #Receta (fórmula)
  Bosque.Aleatorio.receta <- recipe(formula, 
                                    data = datos[train.cv.k_indices,])
  
  #Workflow (modelo y fórmula)
  Bosque.Aleatorio.workflow <- workflow() %>%
    add_model(Bosque.Aleatorio.modelo) %>%
    add_recipe(Bosque.Aleatorio.receta)
  
  #Grid de hiperparámetros
  grid.tun <- grid_random(
    mtry(range = c(5,20)),
    trees(range = c(100,500)),
    min_n(range = c(10,100)),
    size = 20
  )
  
  #Pliegues a usar en el CV del tuneo
  Folds.tun <- vfold_cv(datos[train.cv.k_indices,], v = 3)
  
  #Tuneo de hiperparámetros
  Bosque.Aleatorio.tun <- tune_grid(
    Bosque.Aleatorio.workflow,
    resamples = Folds.tun,
    grid = grid.tun
  )
  
  #Obtener los mejores hiperparámetros
  best.params <- select_best(Bosque.Aleatorio.tun, metric = "accuracy")
  
  #Obtener el mejor modelo
  Bosque.Aleatorio.model.final <- finalize_workflow(Bosque.Aleatorio.workflow, 
                                                best.params)
  
  #Entrenar el modelo final
  Bosque.Aleatorio.model.tun <- fit(Bosque.Aleatorio.model.final, 
                                  data = datos[train.cv.k_indices,])
  
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del pliegue k
  pred.class <- predict(Bosque.Aleatorio.model.tun, 
                        new_data = datos[test.cv.k_indices,]) #predicciones (clases)
  pred.class <- as.vector(pred.class$.pred_class)
  pred.prob <- predict(Bosque.Aleatorio.model.tun, 
                       new_data = datos[test.cv.k_indices,], 
                       type = "prob") #predicciones (probabilidades)
  pred.prob <- as.data.frame(pred.prob)
  y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #valores reales
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,".pred_Poor"],
                pred.Standard = pred.prob[,".pred_Standard"],
                pred.Good = pred.prob[,".pred_Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```


```{r k cross validation Bosque Aleatorio, eval = FALSE}
tic()
set.seed(6)
#formula considerando todas las variables
formula = as.formula("Credit_Score ~ .")

#Evaluar el poder predictivo en cada uno de los K pliegues
Bosque.Aleatorio.CV <- foreach(K = 1:5, 
                      .combine = c, 
                      .packages = c("parsnip","recipes","workflows","tune",
                                    "dials","rsample","yardstick","ranger",
                                    "caret")) %dopar% 
                      {
                      write(paste(Sys.time(), 
                            "Bosque Procesando fold K =", K), 
                            file = "Progreso.txt", #registrar inicio de cada pliegue K
                            append = TRUE) 
                      Bosque.Aleatorio.PP(K, Folds = Folds, #ejecutar la función ára cada pliegue
                             datos=datos[,-c(20:48)], 
                             formula=formula) 
                      }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(Bosque.Aleatorio.CV[[1]], 
                           Bosque.Aleatorio.CV[[3]], 
                           Bosque.Aleatorio.CV[[5]], 
                           Bosque.Aleatorio.CV[[7]], 
                           Bosque.Aleatorio.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(Bosque.Aleatorio.CV[[2]], 
                       Bosque.Aleatorio.CV[[4]], 
                       Bosque.Aleatorio.CV[[6]], 
                       Bosque.Aleatorio.CV[[8]], 
                       Bosque.Aleatorio.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.Bosque.Aleatorio <- Reduce("+", metricas.generales) / n.generales
metricas.clase.Bosque.Aleatorio <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.Bosque.Aleatorio,"metricas.generales.Bosque.Aleatorio.rds")
saveRDS(metricas.clase.Bosque.Aleatorio,"metricas.clase.Bosque.Aleatorio.rds")
```

```{r resultados Bosque Aleatorio, include = TRUE}
metricas.generales.Bosque.Aleatorio <- readRDS("metricas.generales.Bosque.Aleatorio.rds")
metricas.clase.Bosque.Aleatorio <- readRDS("metricas.clase.Bosque.Aleatorio.rds")

t(metricas.generales.Bosque.Aleatorio) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
metricas.clase.Bosque.Aleatorio %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```
Este hasta ahora ha sido el mejor resultado, pues el accuracy aumenta considerablemente a 81%, el Recall de las 3 clases se ubica entre 77% y 84%, con un promedio de 80.7%, donde precisamente la clase Poor que es de nuestro interés alcanza hasta un 84% de Recall, además de que la Precision de la clase Good aumentó mucho hasta 77.4%, y de manera general, la Precision alcanza un 80%. En general, tanto en Recall y Precision, las clases ya se encuentran mucho más equilibradas, la clase Good sigue siendo la de menor valor en ambas (F1 Score de 77.36% respecto al F1 Score de 81.4% de la clase Poor y al F1 Score de 82% de la clase Standard), pero la diferencia ya no es tan significativa como en todos los anteriores modelos, donde había hasta más de un 10% de diferencia en Recall, Precision y F1 Score. 

## Extreme Gradient Boosting

Se consideraron las mismas mallas que se tenían en el bosque aleatorio para los hiperparámetros mtry, trees y min_n, además de tunear la tasa de aprendizaje learn_rate para cada árbol, teniendo entonces los siguientes valores para los hiperparámetros.

- *mtry*: $[5,20]$
- *trees*: $[100,500]$
- *min_n*: $[10,100]$
- *learn_rate*: $[0.1,0.25]$

Cabe mencionar que los hiperparámetros loss_reduction, sample_size y tree_depth se mantienen con los valores por defecto dados por tidymodels (0,1 y 6 respectivamente), ya que al considerar mallas para estos parámetros el modelo tendió a sobreajustar, y por tanto, se obtenían accuracies alrededor de 70%, lo cual no era lo mejor con respecto a los últimos modelos.

```{r poder predictivo XGBoost, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando Extreme Gradient Boosting mediante K Cross Validation
XGBoost.PP = function(k, Folds, datos, formula){
  #Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  
  #Modelo con hiperparámetros a tunear
  XGBoost.modelo <- boost_tree(
    trees = tune(), #número de árboles
    min_n = tune(), #numero minimo de observaciones por nodo final
    mtry = tune(), #variables por árbol
    learn_rate = tune(), #tasa de aprendizaje
  ) %>%
    set_engine("xgboost") %>%
    set_mode("classification")
  
  #Receta (fórmula)
  XGBoost.receta <- recipe(formula, 
                            data = datos[train.cv.k_indices,])
  
  #Workflow (modelo y fórmula)
  XGBoost.workflow <- workflow() %>%
    add_model(XGBoost.modelo) %>%
    add_recipe(XGBoost.receta)
  
  #Grid de hiperparámetros
  grid.tun <- grid_random(
    mtry = mtry(range = c(5L,20L)), 
    trees(range = c(100,500)),
    min_n = min_n(range = c(10L,100L)),
    learn_rate(range = c(0.1,0.25)),           
    size = 20
  )
  
  #Pliegues a usar en el CV del tuneo
  Folds.tun <- vfold_cv(datos[train.cv.k_indices,], v = 3)
  
  #Tuneo de hiperparámetros
  XGBoost.tun <- tune_grid(
    XGBoost.workflow,
    resamples = Folds.tun,
    grid = grid.tun
  )
  
  #Obtener los mejores hiperparámetros
  best.params <- select_best(XGBoost.tun, metric = "accuracy")
  
  #Obtener el mejor modelo
  XGBoost.model.final <- finalize_workflow(XGBoost.workflow, 
                                                best.params)
  
  #Entrenar el modelo final
  XGBoost.model.tun <- fit(XGBoost.model.final, 
                                  data = datos[train.cv.k_indices,])
  
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del pliegue k
  pred.class <- predict(XGBoost.model.tun, 
                        new_data = datos[test.cv.k_indices,]) #predicciones (clases)
  pred.class <- as.vector(pred.class$.pred_class)
  pred.prob <- predict(XGBoost.model.tun, 
                       new_data = datos[test.cv.k_indices,], 
                       type = "prob") #predicciones (probabilidades)
  pred.prob <- as.data.frame(pred.prob)
  y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #valores reales
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,".pred_Poor"],
                pred.Standard = pred.prob[,".pred_Standard"],
                pred.Good = pred.prob[,".pred_Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```


```{r k cross validation XGBoost, eval = FALSE}
tic()
set.seed(7)
#formula considerando todas las variables
formula = as.formula("Credit_Score ~ .")

#Evaluar el poder predictivo en cada uno de los K pliegues
XGBoost.CV <- foreach(K = 1:5, 
                      .combine = c, 
                      .packages = c("parsnip","recipes","workflows","tune","dials",
                                    "rsample","yardstick","xgboost",
                                    "caret")) %dopar% 
                      {
                      write(paste(Sys.time(), 
                            "XGBoost Procesando fold K =", K), 
                            file = "Progreso.txt", #registrar inicio de cada pliegue K
                            append = TRUE) 
                      XGBoost.PP(K, Folds = Folds, #ejecutar la función ára cada pliegue
                             datos=datos[,-c(20:48)], 
                             formula=formula) 
                      }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(XGBoost.CV[[1]], 
                           XGBoost.CV[[3]], 
                           XGBoost.CV[[5]],
                           XGBoost.CV[[7]], 
                           XGBoost.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(XGBoost.CV[[2]], 
                       XGBoost.CV[[4]], 
                       XGBoost.CV[[6]], 
                       XGBoost.CV[[8]], 
                       XGBoost.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.XGBoost <- Reduce("+", metricas.generales) / n.generales
metricas.clase.XGBoost <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.XGBoost,"metricas.generales.XGBoost.rds")
saveRDS(metricas.clase.XGBoost,"metricas.clase.XGBoost.rds")
```

```{r resultados XGBoost, include = TRUE}
metricas.generales.XGBoost <- readRDS("metricas.generales.XGBoost.rds")
metricas.clase.XGBoost <- readRDS("metricas.clase.XGBoost.rds")

t(metricas.generales.XGBoost) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
metricas.clase.XGBoost %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```

Se obtuvo un accuracy cercano al 78%, lo cuál, a pesar de ser Extreme Gradient Boosting un modelo que va mejorando los árboles que se entrenan, no superó al 81% obtenido en bosque aleatorio, aunque por el momento es el segundo mejor modelo obtenido con respecto al Accuracy. Respecto a las métricas específicas por clase, así como el Accuracy disminuyó alrededor de 3% con respecto al bosque aleatorio, el Recall y Precision en general también disminuyeron 3.5% y 2% respectivamente, y sobretodo en las clases Good y Poor en el caso del Recall tenemos un decremento del 6% en Recall, y 3% para Precision en el caso de las clases Good y Standard, dando como resultado que en las 3 clases el F1-Score se redujera alrededor de 3% a 4% con respecto al bosque aleatorio, similar a lo que pasó con el Accuracy. Si bien este modelo es mejor que el segundo mejor modelo que teníamos antes tanto en Accuracy como en las métricas por clase (K-Nearest-Neighbors), no se mejoraron los resultados obtenidos previamente con el bosque aleatorio, muy probablemente debido a que todas las combinaciones de parámetros produjeron un poco de overfitting a comparación del bosque aleatorio, y por tanto, algunos de la clase Poor se clasificaron en Good y de la clase Good en Standard.

## Stacking (KNN, Bosque Aleatorio y Extreme Gradient Boosting)

Finalmente, se consideró hacer un Stacking usando los 3 mejores modelos obtenidos hasta el momento, los cuáles fueron K-Nearest-Neighbors, Bosque Aleatorio y Extreme Gradient Boosting. Se entrenaron los 3 modelos con las mismas mallas de hiperparámetros usadas de manera individual por cada modelo, se obtuvieron las predicciones del conjunto de entrenamiento, y con estas predicciones se entrenó el modelo final Meta, el cuál fue una regresión logística multinomial vía Red Neuronal.

```{r poder predictivo Stacking, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando un Stacking mediante K Cross Validation
Stacking.PP = function(k, Folds, datos, formula){
#Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  
  #-----------------------------KNN--------------------------------------------
  KNN.model.tun <- train(formula, #KNN con tuneo de hiperparámetros
                       data = datos[train.cv.k_indices,],
                       method = "knn",
                       preProcess = c("center","scale"), #estandarización
                       trControl = trainControl(method = "cv", 
                                                number = 3),
                       tuneGrid = expand.grid(k = c(3,5,7))) #malla de k

  #KNN: predicciones de probabilidad
  KNN.pred <- predict(KNN.model.tun, 
                      newdata = datos[train.cv.k_indices,], 
                      type = "prob")
  colnames(KNN.pred) <- paste0("KNN_", colnames(KNN.pred))
  
  
  #-----------------------Bosque Aleatorio-------------------------------------
  Bosque.Aleatorio.modelo <- rand_forest(
    mtry = tune(), #variables por árbol
    trees = tune(), #numero de arboles
    min_n = tune()  #numero minimo de observaciones por nodo final
  ) %>%
    set_engine("ranger") %>%
    set_mode("classification")
  
  #Receta (fórmula)
  Bosque.Aleatorio.receta <- recipe(formula, 
                                    data = datos[train.cv.k_indices,])
  
  #Workflow (modelo y fórmula)
  Bosque.Aleatorio.workflow <- workflow() %>%
    add_model(Bosque.Aleatorio.modelo) %>%
    add_recipe(Bosque.Aleatorio.receta)
  
  #Grid de hiperparámetros
  grid.tun <- grid_random(
    mtry(range = c(5,20)),
    trees(range = c(100,500)),
    min_n(range = c(10,100)),
    size = 20
  )
  
  #Pliegues a usar en el CV del tuneo
  Folds.tun <- vfold_cv(datos[train.cv.k_indices,], v = 3)
  
  #Tuneo de hiperparámetros
  Bosque.Aleatorio.tun <- tune_grid(
    Bosque.Aleatorio.workflow,
    resamples = Folds.tun,
    grid = grid.tun
  )
  
  #Obtener los mejores hiperparámetros
  best.params <- select_best(Bosque.Aleatorio.tun, metric = "accuracy")
  
  #Obtener el mejor modelo
  Bosque.Aleatorio.model.final <- finalize_workflow(Bosque.Aleatorio.workflow, 
                                                best.params)
  
  #Entrenar el modelo final
  Bosque.Aleatorio.model.tun <- fit(Bosque.Aleatorio.model.final, 
                                  data = datos[train.cv.k_indices,])
  
  #Bosque Aleatorio: predicciones de probabilidad
  Bosque.Aleatorio.pred <- predict(Bosque.Aleatorio.model.tun, 
                                   new_data = datos[train.cv.k_indices,], 
                                   type = "prob") 
  Bosque.Aleatorio.pred <- as.data.frame(Bosque.Aleatorio.pred)
  colnames(Bosque.Aleatorio.pred) <- paste0("Bosque.Aleatorio_", 
                                        colnames(Bosque.Aleatorio.pred))
  
  
  #--------------------------------XGBoost-------------------------------------
  XGBoost.modelo <- boost_tree(
    trees = tune(), #número de árboles
    min_n = tune(), #numero minimo de observaciones por nodo final
    mtry = tune(), #variables por árbol
    learn_rate = tune(), #tasa de aprendizaje
  ) %>%
    set_engine("xgboost") %>%
    set_mode("classification")
  
  #Receta (fórmula)
  XGBoost.receta <- recipe(formula, 
                            data = datos[train.cv.k_indices,])
  
  #Workflow (modelo y fórmula)
  XGBoost.workflow <- workflow() %>%
    add_model(XGBoost.modelo) %>%
    add_recipe(XGBoost.receta)
  
  #Grid de hiperparámetros
  grid.tun <- grid_random(
    mtry = mtry(range = c(5L,20L)), 
    trees(range = c(100,500)),
    min_n = min_n(range = c(10L,100L)),
    learn_rate(range = c(0.1,0.25)),           
    size = 20
  )
  
  #Pliegues a usar en el CV del tuneo
  Folds.tun <- vfold_cv(datos[train.cv.k_indices,], v = 3)
  
  #Tuneo de hiperparámetros
  XGBoost.tun <- tune_grid(
    XGBoost.workflow,
    resamples = Folds.tun,
    grid = grid.tun
  )
  
  #Obtener los mejores hiperparámetros
  best.params <- select_best(XGBoost.tun, metric = "accuracy")
  
  #Obtener el mejor modelo
  XGBoost.model.final <- finalize_workflow(XGBoost.workflow, 
                                                best.params)
  
  #Entrenar el modelo final
  XGBoost.model.tun <- fit(XGBoost.model.final, 
                                  data = datos[train.cv.k_indices,])

  # XGBoost: predicciones de probabilidad
  XGBoost.pred <- predict(XGBoost.model.tun, 
                          new_data = datos[train.cv.k_indices,], 
                          type = "prob")
  XGBoost.pred <- as.data.frame(XGBoost.pred)
  colnames(XGBoost.pred) <- paste0("XGB_", colnames(XGBoost.pred))
  
  
  #--------------------------------Modelo Meta---------------------------------
  Stacking.pred <- cbind(
    KNN.pred,
    Bosque.Aleatorio.pred,
    XGBoost.pred,
    Credit_Score = datos[train.cv.k_indices,"Credit_Score"]
  )
  
  Meta.model <- multinom(formula, data = Stacking.pred)
  
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del pliegue k
  
  #predicciones modelos individuales  
  KNN.pred.prob <- predict(KNN.model.tun, 
                       newdata = datos[test.cv.k_indices,], 
                       type = "prob") #probabilidades KNN
  KNN.pred.prob <- as.data.frame(KNN.pred.prob)
  colnames(KNN.pred.prob) <- paste0("KNN_", colnames(KNN.pred.prob))
  
  Bosque.Aleatorio.pred.prob <- predict(Bosque.Aleatorio.model.tun, 
                       new_data = datos[test.cv.k_indices,], 
                       type = "prob") #probabilidades KNN
  Bosque.Aleatorio.pred.prob <- as.data.frame(Bosque.Aleatorio.pred.prob)
  colnames(Bosque.Aleatorio.pred.prob) <- paste0("Bosque.Aleatorio_", 
                                                 colnames(Bosque.Aleatorio.pred.prob))
  
  XGBoost.pred.prob <- predict(XGBoost.model.tun, 
                       new_data = datos[test.cv.k_indices,], 
                       type = "prob") #probabilidades XGBoost
  XGBoost.pred.prob <- as.data.frame(XGBoost.pred.prob)
  colnames(XGBoost.pred.prob) <- paste0("XGB_",colnames(XGBoost.pred.prob))

  #dataframe con las probabilidades por clase de Test de cada modelo
  Stacking.pred.prob <- cbind(
    KNN.pred.prob,
    Bosque.Aleatorio.pred.prob,
    XGBoost.pred.prob,
    Credit_Score = datos[test.cv.k_indices,"Credit_Score"]
  )
  
  #predicciones modelo meta
  pred.class <- predict(Meta.model, 
                        newdata = Stacking.pred.prob) #predicciones (clases)
  pred.prob <- predict(Meta.model, 
                       newdata = Stacking.pred.prob, 
                       type = "prob") #predicciones (probabilidades)
  pred.prob <- as.data.frame(pred.prob)
  y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #valores reales
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,"Poor"],
                pred.Standard = pred.prob[,"Standard"],
                pred.Good = pred.prob[,"Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```

```{r k cross validation Stacking, eval = FALSE}
tic()
set.seed(8)
#formula considerando todas las variables
formula = as.formula("Credit_Score ~ .")

#Evaluar el poder predictivo en cada uno de los K pliegues
Stacking.CV <- foreach(K = 1:5, 
                      .combine = c, 
                      .packages = c("parsnip","recipes","workflows","tune","dials",
                                    "rsample","yardstick","xgboost","ranger",
                                    "caret", "nnet",
                                    "dplyr","tibble","stringr","forcats")) %dopar% 
                      {
                      write(paste(Sys.time(), 
                            "Stacking Procesando fold K =", K), 
                            file = "Progreso.txt", #registrar inicio de cada pliegue K
                            append = TRUE) 
                      Stacking.PP(K, Folds = Folds, #ejecutar la función ára cada pliegue
                             datos=datos[,-c(20:48)], 
                             formula=formula) 
                      }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(Stacking.CV[[1]], 
                           Stacking.CV[[3]], 
                           Stacking.CV[[5]],
                           Stacking.CV[[7]], 
                           Stacking.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(Stacking.CV[[2]], 
                       Stacking.CV[[4]], 
                       Stacking.CV[[6]], 
                       Stacking.CV[[8]], 
                       Stacking.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.Stacking <- Reduce("+", metricas.generales) / n.generales
metricas.clase.Stacking <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.Stacking,"metricas.generales.Stacking.rds")
saveRDS(metricas.clase.Stacking,"metricas.clase.Stacking.rds")
```

```{r resultados Stacking, include = TRUE}
metricas.generales.Stacking <- readRDS("metricas.generales.Stacking.rds")
metricas.clase.Stacking <- readRDS("metricas.clase.Stacking.rds")

t(metricas.generales.Stacking) %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
metricas.clase.Stacking %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```

Notemos que, a pesar de combinar varios modelos, en este caso tampoco logramos superar al Bosque Aleatorio, solo mejoraramos los resultados del Extreme Gradient Boosting aumentando 1% en Accuracy, y alrededor de 1% a 2% en todas las métricas generales por clase, de las cuáles ninguna mejoró con respecto al Bosque Aleatorio. La causa más probable de esto se debe a que tanto KNN como Extreme Gradient Boosting pueden no tener mucha diversidad en sus predicciones (ser similares en cuanto a sus errores) y por tanto, no aportar mucho al Bosque Aleatorio, sobre todo en aquellas observaciones muy difíciles de clasificar correctamente, además de que el modelo Meta usado no es muy complejo.

## Conclusiones

A continuación, se muestra un resumen de las métricas de interés principal (Accuracy, Recall clase Poor y Precision clase Good) obtenidas por todos los modelos, ordenados de manera descendente de acuerdo a su rendimiento.

```{r tabla final, include = TRUE}
#Modelos
Modelo <- c("Bosque Aleatorio","Stacking","Extreme Gradient Boosting",
            "K Nearest Neighbors","Árbol de Decisión","QDA",
            "Regresión Logística Multinomial","LDA","Naive Classifier")

#Accuracy
Accuracy <- c(metricas.generales.Bosque.Aleatorio[[1]],
              metricas.generales.Stacking[[1]],
              metricas.generales.XGBoost[[1]],
              metricas.generales.KNN[[1]],
              metricas.generales.Arbol.Decision[[1]],
              metricas.generales.QDA[[1]],
              metricas.generales.reg.log.mult[[1]],
              metricas.generales.LDA[[1]],
              metricas.generales.Naive[[1]])

#Recall clase Poor
Recall.Poor <- c(metricas.clase.Bosque.Aleatorio[[1]],
                 metricas.clase.Stacking[[1]],
                 metricas.clase.XGBoost[[1]],
                 metricas.clase.KNN[[1]],
                 metricas.clase.Arbol.Decision[[1]],
                 metricas.clase.QDA[[1]],
                 metricas.clase.reg.log.mult[[1]],
                 metricas.clase.LDA[[1]],
                 metricas.clase.Naive[[1]])

#Precision clase Good
Precision.Good <- c(metricas.clase.Bosque.Aleatorio[[9]],
                    metricas.clase.Stacking[[9]],
                    metricas.clase.XGBoost[[9]],
                    metricas.clase.KNN[[9]],
                    metricas.clase.Arbol.Decision[[9]],
                    metricas.clase.QDA[[9]],
                    metricas.clase.reg.log.mult[[9]],
                    metricas.clase.LDA[[9]],
                    metricas.clase.Naive[[9]])

#dataframe con los resultados
resultados <- data.frame(Modelo,Accuracy,Recall.Poor,Precision.Good)
resultados %>% 
  kbl(booktabs = TRUE, align = "c") %>%
  kable_styling(position = "center", latex_options = c("HOLD_position"))
```

Con respecto al baseline de 53% de Accuracy que se obtendría si se clasificaran a todas las observaciones como "Standard" (clase con mayor proporción), tenemos que todos los modelos superaron este baseline, por lo que si aportan algo de valor. Naive Classifier es el peor modelo tanto en Accuracy como en Precision del grupo Good, indicandonos que las observaciones no son facilmente clasificables solo con probabilidad condicional. Por otro lado, la regresión logística multinomial, LDA y QDA mejoran el baseline alrededor de 12% a 13%, sin embargo, Recall de Poor y Precision de Good tienen un valor de alrededor de 50%, salvo QDA con una mejora de 75% de Recall del grupo Poor, indicándonos que las relaciones entre Credit Score y el resto de variables no son solamente lineales ni cuadráticas, sino más complejas.

K Nearest Neighbors y los Árboles de Decisión mejoran considerablemente el baseline a poco más de 20%, encontrando patrones más profundos entre Credit Score y las variables, además de aumentar de igual forma 20% para el Recall de la clase Poor y al menos 10% para Precision de la clase Good. Al introducir variabilidad y mayor aprendizaje con modelos como el Bosque Aleatorio, Extreme Gradient Boosting y el Stacking de KNN, Bosque Aleatorio y Extreme Gradient Boosting, se mejora alrededor de 5% a 7% a las 3 métricas, principalmente Precision de Good, de donde el Bosque Aleatorio termina siendo el modelo más óptimo de todos los modelos entrenados al ser el único que obtuvo un Accuracy mayor al 80%, así como al aumentar el Recall del grupo Poor un 5% con respecto al segundo mejor modelo (Stacking), y aumentar un 2% tanto en Accuracy como en Precision de Good respecto a ese segundo mejor modelo.

Es importante destacar que no necesariamente el Bosque Aleatorio es el mejor modelo definitivo, muy probablemente se pueden mejorar ligeramente las métricas con otros modelos como Support Vector Machine o Redes Neuronales, los cuales podrían agregarse al Stacking junto con un modelo Meta más complejo para tener más variabilidad y aprendizaje. De iual manera, los modelos probados podrían optimizarse con un tuneo más profundo o inteligente de hiperparámetros como una búsqueda completa en las mallas dadas o una búsqueda Bayesiana. Debido al gran costo computacional que implica el entrenamiento de todas estas alternativas, no se incluyeron en este proyecto, pero es importante mencionarlas ya que los modelos obtenidos se pueden mejorar. 

```{r poder predictivo SVM, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando Support Vector Machine mediante K Cross Validation
SVM.PP = function(k, Folds, datos, formula){
  #Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  train.cv.k_indices <- c(sample(which(datos[train.cv.k_indices,]$Credit_Score == "Good"), 1440), 
                          sample(which(datos[train.cv.k_indices,]$Credit_Score == "Standard"), 4240), 
                          sample(which(datos[train.cv.k_indices,]$Credit_Score == "Poor"), 2320))
  SVM.model.tun <- train(formula, #SVM con tuneo de hiperparámetros
                       data = datos[train.cv.k_indices,],
                       method = "svmRadial",
                       trControl = trainControl(method = "cv",
                                                number = 3, 
                                                classProbs = TRUE),
                       preProcess = c("center","scale"), #estandarización
                       tuneGrid = expand.grid(C = c(100), #malla de C
                                            sigma = c(0.5))) #malla de sigma
  
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del pliegue k
  test.cv.k_indices <- c(sample(which(datos[test.cv.k_indices,]$Credit_Score == "Good"), 360), 
                          sample(which(datos[test.cv.k_indices,]$Credit_Score == "Standard"), 1060), 
                          sample(which(datos[test.cv.k_indices,]$Credit_Score == "Poor"), 580))
  pred.class <- predict(SVM.model.tun, 
                        newdata = datos[test.cv.k_indices,]) #predicciones (clases)
  pred.prob <- predict(SVM.model.tun, 
                       newdata = datos[test.cv.k_indices,], 
                       type = "prob") #predicciones (probabilidades)
  pred.prob <- as.data.frame(pred.prob)
  y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #valores reales
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,"Poor"],
                pred.Standard = pred.prob[,"Standard"],
                pred.Good = pred.prob[,"Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```

```{r k cross validation SVM, eval = FALSE}
tic()
#formula considerando todas las variables
formula = as.formula("Credit_Score ~ .")

#Evaluar el poder predictivo en cada uno de los K pliegues
SVM.CV <- foreach(K = 1:5, 
                  .combine = c, 
                  .packages = c("yardstick", "caret", "kernlab")) %dopar% 
                  {
                  write(paste(Sys.time(), 
                        "SVM Procesando fold K =", K), 
                        file = "Progreso.txt", #registrar inicio de cada pliegue K
                        append = TRUE) 
                  SVM.PP(K, Folds = Folds, #ejecutar la función ára cada pliegue
                          datos=datos[,-c(20:48)], 
                          formula=formula) 
                      }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(SVM.CV[[1]], 
                           SVM.CV[[3]], 
                           SVM.CV[[5]], 
                           SVM.CV[[7]], 
                           SVM.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(SVM.CV[[2]], 
                       SVM.CV[[4]], 
                       SVM.CV[[6]], 
                       SVM.CV[[8]], 
                       SVM.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.SVM <- Reduce("+", metricas.generales) / n.generales
metricas.clase.SVM <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.SVM,"metricas.generales.SVM.rds")
saveRDS(metricas.clase.SVM,"metricas.clase.SVM.rds")
```

```{r resultados SVM, eval = FALSE}
metricas.generales.SVM <- readRDS("metricas.generales.SVM.rds")
metricas.clase.SVM <- readRDS("metricas.clase.SVM.rds")

metricas.generales.SVM
metricas.clase.SVM
```


```{r poder predictivo Red Neuronal, eval = FALSE}
#Función para el cálculo de métricas de proder predictivo usando Red Neuronal mediante K Cross Validation
Red.Neuronal.PP = function(k, Folds, datos, formula){
  #Train CV
  train.cv.k_indices <- which(Folds != k) #indices del train del pliegue k
  Red.Neuronal.tun <- train(formula,
                      data = datos[train.cv.k_indices,],
                      method = "nnet",
                      preProcess = c("center","scale"),
                      trControl = trainControl(method = "cv",
                                               number = 3),
                      tuneGrid = expand.grid(size = c(20,30,40), #número de neuronas ocultas
                                    decay = c(0.001,0.01,0.1)), #regularización
                      trace = TRUE,
                      maxit = 700)
  
  #Test CV
  test.cv.k_indices <- (-train.cv.k_indices) #indices del test del pliegue k
  pred.class <- predict(Red.Neuronal.tun, 
                        newdata = datos[test.cv.k_indices,]) #predicciones (clases)
  pred.prob <- predict(Red.Neuronal.tun, 
                       newdata = datos[test.cv.k_indices,], 
                       type = "prob") #predicciones (probabilidades)
  pred.prob <- as.data.frame(pred.prob)
  y.test.cv <- datos[test.cv.k_indices,"Credit_Score"] #valores reales
  
  #dataframe comparativo real vs prediccion (para obtener métricas)
  df.pred <- data.frame(
                real = y.test.cv,
                pred = ordered(pred.class, 
                               levels = c("Poor", "Standard", "Good")),
                real.Poor = factor(as.numeric(y.test.cv == "Poor"), 
                                   levels = c("1","0")),
                real.Standard = factor(as.numeric(y.test.cv == "Standard"), 
                                       levels = c("1","0")),
                real.Good = factor(as.numeric(y.test.cv == "Good"), 
                                   levels = c("1","0")),
                pred.Poor = pred.prob[,"Poor"],
                pred.Standard = pred.prob[,"Standard"],
                pred.Good = pred.prob[,"Good"])
  
  #métricas generales (promedio de las 3 clases)
  accuracy <- yardstick::accuracy(df.pred, truth = real, estimate = pred, 
                       estimator = "macro")
  recall <- yardstick::recall(df.pred, truth = real, estimate = pred, 
                   estimator = "macro")
  specificity <- yardstick::specificity(df.pred, truth = real, estimate = pred, 
                             estimator = "macro")
  precision <- yardstick::precision(df.pred, truth = real, estimate = pred, 
                         estimator = "macro")
  f1 <- yardstick::f_meas(df.pred, truth = real, estimate = pred, 
               estimator = "macro")
  roc_auc <- yardstick::roc_auc(df.pred, truth = real, 
             pred.Poor, pred.Standard, pred.Good,
             estimator = "macro")
  
  #matriz de confusión
  conf_mat <- confusionMatrix(data = ordered(pred.class, 
                                      levels = c("Poor", "Standard", "Good")),
                            reference = y.test.cv)
  
  #roc_auc por clase (unica metrica que no esta en la matriz de confusión)
  roc_auc_Poor <- yardstick::roc_auc(df.pred, truth = real.Poor, pred.Poor)
  roc_auc_Standard <- yardstick::roc_auc(df.pred, truth = real.Standard, pred.Standard)
  roc_auc_Good <- yardstick::roc_auc(df.pred, truth = real.Good, pred.Good)
  
  #vector final de métricas generales
  metricas.generales <- c(accuracy = accuracy$.estimate, 
                          recall = recall$.estimate, 
                          specificity = specificity$.estimate, 
                          precision = precision$.estimate, 
                          f1 = f1$.estimate, roc_auc = roc_auc$.estimate)
  
  #dataframe final de métricas por clase
  metricas.clase <- cbind(conf_mat$byClass[,c("Recall","Specificity",
                                              "Precision","F1")],
                       ROC_AUC = c(roc_auc_Poor$.estimate,
                       roc_auc_Standard$.estimate,
                       roc_auc_Good$.estimate))
  
  return(list(
    metricas.generales = metricas.generales,
    metricas.clase = metricas.clase
  ))
}
```

```{r k cross validation Red Neuronal, eval = FALSE}
tic()
#formula considerando todas las variables
formula = as.formula("Credit_Score ~ .")

#Evaluar el poder predictivo en cada uno de los K pliegues
Red.Neuronal.CV <- foreach(K = 1:5, 
                  .combine = c, 
                  .packages = c("yardstick", "caret", "nnet")) %dopar% 
                  {
                  write(paste(Sys.time(), 
                        "Red.Neuronal Procesando fold K =", K), 
                        file = "Progreso.txt", #registrar inicio de cada pliegue K
                        append = TRUE) 
                  Red.Neuronal.PP(K, Folds = Folds, #ejecutar la función ára cada pliegue
                          datos=datos[,-c(20:48)], 
                          formula=formula) 
                      }
toc()

#Métricas generales por cada pliegue
metricas.generales <- list(Red.Neuronal.CV[[1]], 
                           Red.Neuronal.CV[[3]], 
                           Red.Neuronal.CV[[5]], 
                           Red.Neuronal.CV[[7]], 
                           Red.Neuronal.CV[[9]])

#Métricas por clase para cada pliegue
metricas.clase <- list(Red.Neuronal.CV[[2]], 
                       Red.Neuronal.CV[[4]], 
                       Red.Neuronal.CV[[6]], 
                       Red.Neuronal.CV[[8]], 
                       Red.Neuronal.CV[[10]])

#Obtener el promedio por métrica de los K Cross Validation
n.generales <- length(metricas.generales)
n.clase <- length(metricas.clase)

metricas.generales.Red.Neuronal <- Reduce("+", metricas.generales) / n.generales
metricas.clase.Red.Neuronal <- Reduce("+", metricas.clase) / n.clase

saveRDS(metricas.generales.Red.Neuronal,"metricas.generales.Red.Neuronal.rds")
saveRDS(metricas.clase.Red.Neuronal,"metricas.clase.Red.Neuronal.rds")
```

```{r resultados Red Neuronal, eval = FALSE}
metricas.generales.Red.Neuronal <- readRDS("metricas.generales.Red.Neuronal.rds")
metricas.clase.Red.Neuronal <- readRDS("metricas.clase.Red.Neuronal.rds")

metricas.generales.Red.Neuronal
metricas.clase.Red.Neuronal
```

